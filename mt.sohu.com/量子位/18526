➜1500元就能买一只机器狗，南京大学校友制作，可跑可跳可爬坡，踩一脚不会挂的那种
http://www.sohu.com/a/419822647_610300	16042
<p>杨净 发自 凹非寺 </p>
<p>量子位 报道 | 公众号 QbitAI </p>
<p style="text-align: left;">你是否也曾想拥有一只宠物机器狗？现在有一只在你面前。</p>
<p style="text-align: center;"><img src="http://p4.itc.cn/q_70/images03/20200921/2156c78f5687447fb609265f694651f4.gif" /></p>
<p style="text-align: left;">只有手掌大小。</p>
<p style="text-align: center;"><img src="http://p6.itc.cn/q_70/images03/20200921/2700b5487f2c43a3956e3bb29fefb763.jpeg" /></p>
<p style="text-align: left;">可跑可跳，还可以爬上纸箱子。如果摔倒了，就自己爬起来。</p>
<p style="text-align: left;">就连在下雨天想走，沙地上也都完全没问题。</p>
<p style="text-align: center;"><img src="http://p6.itc.cn/q_70/images03/20200921/5c188333514c40379940d67200156f1c.gif" /></p>            <div class="lookall-box">
<div class="lookall-shadow"></div>
<section class="lookall">
<a href="javascript:;" class="show-all" id="showMore">
<em>展开全文</em>
</a>
</section>
</div>
<div class="hidden-content control-hide">
<p style="text-align: left;">即便是踩一脚，也都能「励志」地站起来，不会挂的那种。</p>
<p style="text-align: center;"><img src="http://p8.itc.cn/q_70/images03/20200921/32afbd9ff41f46adbad76b51ee701613.gif" /></p>
<p style="text-align: left;">还可以像逗猫一样…</p>
<p style="text-align: center;"><img src="http://p8.itc.cn/q_70/images03/20200921/2b7f078eee9e4b28b3975e0f74f8c74f.gif" /></p>
<p style="text-align: left;">嗯…跟52万的波士顿机器狗比起来，价格确实比较美丽。</p>
<p style="text-align: left;">这款机器人是来自一个众筹项目，名叫Petoi Bittle，项目负责人是南京大学校友 <strong>李荣仲</strong>。 </p>
<p style="text-align: center;"><img src="http://p8.itc.cn/q_70/images03/20200921/3e1c922466d24e2780e8905b88b09162.png" /></p>
<p>量产低价的迷你机器狗 </p>
<p style="text-align: left;">据官网介绍，他们致力于制作适合「所有人」的量产低成本的机器狗。</p>
<p style="text-align: left;">这个机器狗Bittle，尺寸为20cm×11cm×11cm，重量不到280g，它的肚子下可以携带450g的「货物」，最快奔跑速度，可以达到每秒两个身长。</p>
<p style="text-align: left;">此外，Bittle可以记住数十种本能的运动模式，并能在实时指令下表演更多的花样。</p>
<p style="text-align: left;">它由五个主要部件组成：车体框架、执行器、电子元件、电池和软件。</p>
<p style="text-align: left;"><strong>车体框架</strong>，用户可以像组装3D拼图一样拼装Bittle，整个框架制作过程大概需要1小时。 </p>
<p style="text-align: center;"><img src="http://p2.itc.cn/q_70/images03/20200921/9922fbdd407649558e5c35d7a2d9a30d.jpeg" /></p>
<p style="text-align: left;"><strong>执行器</strong>。 </p>
<p style="text-align: center;"><img src="http://p5.itc.cn/q_70/images03/20200921/86e4aec905164dd7969f1cce62d2200b.jpeg" /></p>
<p style="text-align: left;">整个机器人需要9台P1S伺服系统来驱动，其中8台用于行走关节，1台用于云台平移。当然，还会附赠一个备用伺服。</p>
<p style="text-align: left;"><strong>开发板</strong>。 </p>
<p style="text-align: left;">Bittle是由NyBoard V1驱动的，它是一块定制的Arduino板，用来协调复杂的运动。</p>
<p style="text-align: center;"><img src="http://p6.itc.cn/q_70/images03/20200921/09229675875a4fc1b15a078193dedb0c.png" /></p>
<p style="text-align: left;">用户还可以通过有线或无线的连接，安装树莓派或其他AI芯片，给Bittle注入人工智能机器学习的本领。</p>
<p style="text-align: left;">此外，还在狗的嘴巴里设置了夹子，用来放置可扩展模块，比如，已经研发出来的智能相机模块、手势传感器、PIR传感器等。</p>
<p style="text-align: left;">就像这样。</p>
<p style="text-align: center;"><img src="http://p1.itc.cn/q_70/images03/20200921/7eb58eb985e84dd5b968958b4dd2d8fa.jpeg" /></p>
<p style="text-align: left;">目前，这个项目已经开源，用户可从GitHub上下载演示代码，还可以通过编程接口教会他更多的新技能。</p>
<p>创始人是南大校友 </p>
<p style="text-align: left;">李荣仲，本科就读于南京大学物理系，随后前往美国维克森林大学攻读物理博士和计算机硕士学位。</p>
<p style="text-align: center;"><img src="http://p3.itc.cn/q_70/images03/20200921/62be62ed8751445f823c3bd588e8b2e0.jpeg" /></p>
<p style="text-align: left;">毕业后，留校任兼职助理教书2年，并研发了OpenCat项目，全球首款用消费级舵机实现哺乳类四足步态并量产的机器人平台。</p>
<p style="text-align: center;"><img src="http://p6.itc.cn/q_70/images03/20200921/9f40c12497394da9b21406847f8a98fc.jpeg" /></p>
<p><strong>△</strong>OpenCat </p>
<p style="text-align: left;">随后在第二年，OpenCat系列首款产品机器猫Nybble登陆国际众筹平台Indiegogo，成功筹得25万美元，用户覆盖人群触达全球五十多个国家。</p>
<p style="text-align: center;"><img src="http://p4.itc.cn/q_70/images03/20200921/fa7fe530cf294227a2318e1cb079bbb5.jpeg" /></p>
<p><strong>△</strong>Nybble </p>
<p style="text-align: left;">2019年，回到深圳创办了派拓艺（深圳）科技有限责任公司。</p>
<p style="text-align: left;">目前，这一新的机器狗项目，已募集365,724美元，已经远远超过总目标的50,000美元。</p>
<p style="text-align: left;">你对这款机器狗感兴趣么？</p>
<p style="text-align: left;"><span>众筹链接：</span></p>
<p>https://www.kickstarter.com/projects/petoi/bittle </p>
<p><span>参考链接：</span></p>
<p><span>https://mp.weixin.qq.com/s/86rXEX7eZaB1jQXHVgq18A </span></p>
<p>— <strong>完</strong>— </p>
<p><span>本文系网易新闻•网易号特色内容激励计划签约账号【量子位】原创内容，未经账号授权，禁止随意转载。</span><span style="font-size: 16px;"></span></p>
<p><strong>好课推荐 | 0基础学Python</strong></p>
<p><span style="font-size: 16px;">《动手学Python》课程由</span><strong><span style="font-size: 16px;">上海交大博士教研团队</span></strong><span style="font-size: 16px;">研发，在线学习平台由上海</span><strong><span style="font-size: 16px;">交大AI实验室</span></strong><span style="font-size: 16px;">技术支持，旷视、文远知行、图森未来、来也科技等</span><strong><span style="font-size: 16px;">知名AI</span></strong><span style="font-size: 16px;"><strong>企业CEO/CTO力荐</strong></span><span style="font-size: 16px;">。</span></p>
<p style="text-align: left;"><span style="font-size: 16px;">0基础可入门，限时开放体验课！扫码即刻开启Python学习之旅：</span></p>
<p><span style="font-size: 16px;"><strong>量子位 </strong></span><span style="font-size: 16px;">QbitAI · 头条号签约作者</span></p>
<p>վ'ᴗ' ի 追踪AI技术和产品新动态</p>
<p><span>一键三连「分享」、「点赞」和「在看」</span></p>
<p><span>科技前沿进展日日相见~</span></p>
➜IOI 2020落幕，中国队团体成绩第一，美籍华裔选手拿下唯一满分
http://www.sohu.com/a/419830250_610300	16042
<p>边策 贾浩楠 发自 凹非寺 </p>
<p>量子位 报道 | 公众号 QbitAI </p>
<p style="text-align: left;">2020年国际信息学奥赛（IOI 2020）完成了第二日比赛，四名中国队选手皆进入前十，分列3~7名，团队总成绩第一！</p>
<p><img height="auto" width="1370" src="http://p4.itc.cn/q_70/images03/20200921/1c6b35e2d16c4aba9d6bbeb1a03072c2.png" /></p>
<ul>
<li>罗煜翔：来自宁波市镇海中学，总分592.62，第3名</li>
<li>王展鹏：来自绍兴市第一中学，总分592.62，第3名</li>
<li>周雨扬：来自绍兴市第一中学，总分592.62，第3名</li>
<li>蒋明润：来自成都市第七中学，总分592.24，第7名</li>
</ul>
<p style="text-align: left;">今天在IOI 2020官网上公布的是初步成绩，接下来还会经历成绩审核等环节，若最终确认有效，这四名国家队选手将全部获得IOI金牌。</p>
<p style="text-align: left;">去年和前年的IOI冠军都被华人美国队选手拿走。今年也不例外，IOI 2020第一名仍是一位美籍华人William Lin。</p>
<p><img height="auto" width="1484" src="http://p6.itc.cn/q_70/images03/20200921/5fd91f080d9a404fb421f7daabc5a123.png" /></p>
<p style="text-align: left;">他在2天6道比赛题上全部拿到满分，以总分600分获得冠军，也是今年唯一一位满分选手。IOI继2015年后再次出现满分成绩。William Lin去年也曾参加过IOI，并获得了银牌。</p>
<p style="text-align: left;">中国队去年在IOI上的成绩是三金一银，分别是4、5、20、21名，今年中国队成绩有了大幅提高。</p>            <div class="lookall-box">
<div class="lookall-shadow"></div>
<section class="lookall">
<a href="javascript:;" class="show-all" id="showMore">
<em>展开全文</em>
</a>
</section>
</div>
<div class="hidden-content control-hide">
<p>比赛内容 </p>
<p style="text-align: left;">今年由于受到新冠疫情影响，原本计划在新加坡举办的IOI转移到线上进行，且必须使用官方指定的虚拟机来运行程序。</p>
<p style="text-align: left;">IOI 2020于9月13日开幕，比赛分别在9月16日和9月19日两天举行。根据IOI比赛规定，参赛选手需要在两个比赛日中，每日用5小时解决3道题目。</p>
<p style="text-align: left;">第一日考题为：</p>
<p><img height="auto" width="912" src="http://p0.itc.cn/q_70/images03/20200921/a724b2d005494ac2aedf3ccce01d0857.png" /></p>
<p style="text-align: left;">第二日考题为：</p>
<p><img height="auto" width="900" src="http://p6.itc.cn/q_70/images03/20200921/d8d552c5287048b68116b414b7ba686a.png" /></p>
<p style="text-align: left;">其中，“数蘑菇”问题是拉开前十选手排名的关键题目，在所有参赛选手中，只有第一名，也就是美国队的William Lin拿到了这道题的100分。</p>
<p style="text-align: left;">这道题具体是这样的：</p>
<p><img height="auto" width="1820" src="http://p7.itc.cn/q_70/images03/20200921/c2f538c92d9045e7b7d8e05fc4f92a6d.png" /></p>
<p style="text-align: left;">有兴趣的朋友可以尝试一个如何给这道题编程。</p>
<p style="text-align: left;">IOI 2020的完整考题可以移步文末网址下载，或者在我们的公众号中回复 <strong>IOI</strong>获取。 </p>
<p>IOI中国队 周雨扬 </p>
<p style="text-align: center;"><img src="http://p5.itc.cn/q_70/images03/20200921/319d223ceb624bd78114afa191c2e5d7.png" /></p>
<p style="text-align: left;">周雨扬，高中就读于信息学科竞赛强校 <strong>绍兴一中</strong>。 </p>
<p style="text-align: left;">作为IOI中国队成员，拿下第三名的成绩，其实并不是周雨扬第一次“抛头露面”。</p>
<p style="text-align: left;">早在2017年4月，周雨扬与高中生们同场竞技，参加全国青少年信息学奥林匹克联赛浙江省队选拔赛。</p>
<p style="text-align: center;"><img height="247px" width="242px" src="//p9.itc.cn/q_70/images03/20200921/5363f4041cf2444b85e628836239d004.png" /></p>
<p style="text-align: left;">比赛成绩一出，他就被北大预录取了， <strong>这时，他还在上初三</strong>。 </p>
<p style="text-align: left;">今年的7月20日，37届全国青少年信息学奥林匹克竞赛在广州落下帷幕，上高二的周雨扬以总分625分的成绩获得金牌第一名，正式锁定了北京大学保送资格。</p>
<p style="text-align: left;">同时，他也入选本次国际信息学奥林匹克竞赛国家集训队。</p>
<p style="text-align: left;">初三北大预录取，高二获得保送资格，周雨扬的成绩和实力，在“学霸”中，也算佼佼者了。</p>
<p style="text-align: left;">周雨扬在之前接受《绍兴在线》采访时 <span style="font-size: 16px;">说</span>，搞信息学纯粹是出于兴趣。 </p>
<p style="text-align: left;">小学四年级时，在鲁迅小学就读的他加入了学校的计算机兴趣小组，一开始就从编程学起。</p>
<p style="text-align: left;">“不喜欢打游戏，因为打不过别人。”后来越编越入迷，也越钻越深，“把一个难题做出来，特别开心。”他说。</p>
<p style="text-align: left;">老师绍兴一中信息学指导老师这样评价他：有坚强的意志力，也有超强的自制力，还能不断明晰自己的目标，并为之努力。</p>
<p>王展鹏 </p>
<p style="text-align: center;"><img src="http://p5.itc.cn/q_70/images03/20200921/7568087e0f33435bbc295c97662964e5.png" /></p>
<p style="text-align: left;">王展鹏，同样是绍兴一中学子，和周雨扬一样，在今年的NOI上一起入选国家队，并且还获得了NOI金牌第三名。 </p>
<p><img height="auto" width="626" src="http://p1.itc.cn/q_70/images03/20200921/f6650140b389400b9078cdcbd114dee4.png" /></p>
<p><strong>△</strong>右二为王展鹏，图片来自NOI 2020 </p>
<p style="text-align: left;">还在上高二的王展鹏同样获得计算机学科人才培养一流水平的北京大学英才班入选资格。</p>
<p style="text-align: left;">有如此耀眼的成绩，王展鹏却极少接受采访或在其他场合谈及自己。</p>
<p style="text-align: left;">目前，能找的公开信息，也只有他在今年NOI上获得金牌第三名和去年NOI获得32名。</p>
<p>罗煜翔 </p>
<p style="text-align: center;"><img src="http://p9.itc.cn/q_70/images03/20200921/d7469a86fdcb49f0a1f537e55c6bb8ff.png" /></p>
<p style="text-align: left;">来自宁波镇海中学的罗煜翔同学，是一位“双科”国家队。</p>
<p style="text-align: left;">2018年11月，还在上高一的罗煜翔同学，参加第34届数学奥林匹克决赛获金牌，并入选国家集训队大名单。</p>
<p style="text-align: center;"><img height="auto" width="500" src="http://p6.itc.cn/q_70/images03/20200921/0e5a311921c64cf8882a214aefcb4e04.png" /></p>
<p><strong>△</strong>左一为罗煜翔，图片来自NOI 2019 </p>
<p style="text-align: left;">时隔半年，这位大神又在另一学科——信息学奥林匹克竞赛中，杀入国家集训队。</p>
<p style="text-align: left;">今年的NOI 2020，罗同学又一次进入国家队，并且在IOI上取得了优异成绩。</p>
<p style="text-align: left;">进入国家集训队，就意味着可以直接保送北京大学、清华大学等。</p>
<p style="text-align: left;"><span>而镇海中学这位高</span>二 <span>学生居然能double，而且在不同的学科领域！普通的学霸见了他，可能都会惊叹吧。</span></p>
<p style="text-align: left;"><span>根据 <span style="font-size: 16px;">罗煜翔所在的镇海中学校报的信息，</span></span><span>罗煜翔</span><span>初中就读于蛟川书院，初二的时候就显现出在数学上的天赋。</span></p>
<p style="text-align: left;">初二下学期加入校内的奥数竞赛队。当时，指导老师让大家回去做一套有关平面几何的奥赛数学题，只有初二的罗煜翔是当时唯一一个把这套题几乎都解出来的学生。</p>
<p style="text-align: left;">这套题其实非常难，队里还有很多高一高二的学生都解不出来。</p>
<p style="text-align: left;">罗煜翔自己则说：“要感谢《九章算术》把我带入数学王国，从此开启了我在数学世界的遨游。我最喜欢的读物就是数学书籍，我最崇拜的偶像是古今中外的伟大数学家，我最愉悦的事情是解数学难题。”</p>
<p style="text-align: left;">如此喜爱数学的罗煜翔同学，已经迈出了追求理想的第一步，2019年第一次入选数学国家队时，他就已经被保送了北大数学系。</p>
<p style="text-align: left;">这次的代表信息学国家队出战，可能是罗同学拓展能力兴趣的一次新尝试吧。</p>
<p>蒋明润 </p>
<p style="text-align: center;"><img src="http://p2.itc.cn/q_70/images03/20200921/8f71d14372fa447e8159cefec1f3313a.png" /></p>
<p style="text-align: left;">蒋明润，也是这次NOI中国队里唯一一个不是来自浙江的选手。</p>
<p style="text-align: left;">在此次第37届全国青少年信息学奥林匹克竞赛中，蒋明润以583的成绩获得金牌第4名。</p>
<p style="text-align: left;">根据成都七中官方的报道，蒋明润是继王小川、王修涵之后，该校信息竞赛队员第三次入选国家队。</p>
<p style="text-align: left;"><img height="auto" width="640" src="http://p5.itc.cn/q_70/images03/20200921/52afb8cd28b74f1a93b5234d2cda6856.png" /></p>
<p style="text-align: left;">在高一时，在NOI 2019比赛中，他就获得12名的成绩。</p>
<p style="text-align: left;">初三时，他曾荣获2017NOIP（全国青少年信息学奥林匹克联赛）提高组一等奖。</p>
<p style="text-align: left;">这一次这四位入选国家队的成员，其余三人都去了北大，只有蒋明润去了清华。</p>
<p style="text-align: left;">最后来一张IOI 2020国家队四位选手合影：</p>
<p><img height="auto" width="639" src="http://p5.itc.cn/q_70/images03/20200921/6086cf5739a7433c9d1b37d6a25d31eb.png" /></p>
<p><strong>△</strong>左二起依次为周雨扬、罗煜翔、蒋明润、王展鹏 IOI金牌“圈子” </p>
<p style="text-align: left;">IOI金牌，是我们熟悉的不少国内互联网大佬、技术强人在成长经历中的“标配”。</p>
<p style="text-align: left;">比如来自成都七中、搜狗CEO <strong>王小川</strong>，1996年因获得国际奥林匹克信息学竞赛金牌被报送清华大学计算机系。 </p>
<p style="text-align: left;">还有 <strong>楼天城</strong>，这位曾经百度曾经最年轻的T10级员工，一直致力于无人驾驶，离开百度后，创办了小马智行。 </p>
<p style="text-align: left;">保送清华、江湖人称“楼教主”的楼天城，2004年5月国家队选拔赛第一名，2004年9月获得IOI金牌。</p>
<p style="text-align: left;">在学界，还有2004年，入选国家队获得第6届IOI金牌的 <strong>鬲融</strong>，2004年从河北省保送至清华大学计算机系，后进入“清华姚班”。 </p>
<p style="text-align: left;">2008年毕业后赴美国留学，并在普林斯顿大学攻读博士，后在微软研究院新英格兰分部做博士后，2015年至今在杜克大学担任助理教授。</p>
<p style="text-align: left;">鬲融博士的研究方向是计算机科学近似算法，后来转到理论机器学习方向。</p>
<p style="text-align: left;">以上说到的大佬，都是IOI金牌，保送清华的。</p>
<p style="text-align: left;">清华姚班，也成了清华计算机和信息学科的最响亮的招牌。</p>
<p style="text-align: left;">但这两年，北大的“英才班”也越来越能打。</p>
<p style="text-align: left;">比如2018年拿下IMO金牌的4位中国选手中，除了李一笑，欧阳泽轩和王泽宇同样因为数学选择了北大。</p>
<p style="text-align: left;">2019年在第60届IMO中斩获金牌的6位中国选手中，俞然枫、胡苏麟、黄嘉俊都先后进入了北大数学英才班。</p>
<p style="text-align: left;">今年，4人中的3个全都选了北大，可以在计算机和数学英才班之间选择。</p>
<p style="text-align: left;">无论是北大英才班，还是清华姚班/智班，核心起点都是吸收全国最顶尖的数理人才，再配备最好的师资和教育，帮助他们在本科就能充分夯实基础、发掘潜力。</p>
<p style="text-align: left;">至少现在，清华姚班有了成果，北大数学也诞生黄金一代。</p>
<p style="text-align: left;"><span style="font-size: 16px;">IOI 2020排名： </span></p>
<p>https://ranking.ioi2020.sg/ </p>
<p style="text-align: left;"><span style="font-size: 16px;">IOI 2020考题下载： </span></p>
<p>https://ioi2020.sg/ioi-2020-tasks/ </p>
<p>— <strong>完</strong>— </p>
<p><span>本文系网易新闻•网易号特色内容激励计划签约账号【量子位】原创内容，未经账号授权，禁止随意转载。</span></p>
<p><strong>好课推荐 | 0基础学Python</strong></p>
<p><span style="font-size: 16px;">《动手学Python》课程由</span><strong><span style="font-size: 16px;">上海交大博士教研团队</span></strong><span style="font-size: 16px;">研发，在线学习平台由上海</span><strong><span style="font-size: 16px;">交大AI实验室</span></strong><span style="font-size: 16px;">技术支持，旷视、文远知行、图森未来、来也科技等</span><strong><span style="font-size: 16px;">知名AI</span></strong><span style="font-size: 16px;"><strong>企业CEO/CTO力荐</strong></span><span style="font-size: 16px;">。</span></p>
<p style="text-align: left;"><span style="font-size: 16px;">0基础可入门，限时开放体验课！扫码即刻开启Python学习之旅：</span></p>
<p><span style="font-size: 16px;"><strong>量子位 </strong></span><span style="font-size: 16px;">QbitAI · 头条号签约作者</span></p>
<p>վ'ᴗ' ի 追踪AI技术和产品新动态</p>
<p><span>一键三连「分享」、「点赞」和「在看」</span></p>
<p><span>科技前沿进展日日相见~</span></p>
➜最新Transformer模型大盘点，NLP学习必备，Google AI研究员出品丨资源
http://www.sohu.com/a/419960802_610300	16042
<p>萧箫 发自 凹非寺 </p>
<p>量子位 报道 | 公众号 QbitAI </p>
<p style="text-align: left;">可高效处理长文本的模型Longformer、和堪称 <span style="font-size: 16px;">“升级版</span><span style="font-size: 16px;">”T</span><span style="font-size: 16px;">ransformer</span>的BigBird模型，到底有什么区别？ </p>
<p style="text-align: center;"><img src="http://p6.itc.cn/q_70/images03/20200922/7317273a831f41658449ade0b6764d45.png" /></p>
<p style="text-align: left;">Transformer的其他各种变体 <span>（X-former）</span>到底都长什么样、又有哪些新应用？ </p>
<p style="text-align: left;">由于Transformer模型的发展速度日新月异，一天一个样，哪怕是隔段时间回来研究，模型可能也已经多了不少。</p>
<blockquote>
<p style="text-align: left;">Transformer模型，是谷歌在2017年推出的NLP经典模型 <span>（Bert就是用的Transformer）</span>。 </p>
<p style="text-align: left;">在机器翻译任务上，Transformer表现超过了RNN和CNN，只需要编/解码器就能达到很好的效果，可以高效地并行化。</p>
</blockquote>
<p style="text-align: left;">Transformer模型，是谷歌在2017年推出的NLP经典模型 <span>（Bert就是用的Transformer）</span>。 </p>
<p style="text-align: left;">在机器翻译任务上，Transformer表现超过了RNN和CNN，只需要编/解码器就能达到很好的效果，可以高效地并行化。</p>
<p style="text-align: left;">好消息是，这里有一篇Transformer模型的“最新动向”，它集中探讨Transformer新模型对于自注意力机制 <span>（Self-attention）</span>的改进，并对这些模型进行对比。 </p>
<p style="text-align: left;">此外，还有模型在NLP、计算机视觉和强化学习等各个领域的最新应用。</p>
<p>标准Transformer模型 </p>
<p style="text-align: left;">首先来看看，标准的Transformer模型是什么样的。</p>            <div class="lookall-box">
<div class="lookall-shadow"></div>
<section class="lookall">
<a href="javascript:;" class="show-all" id="showMore">
<em>展开全文</em>
</a>
</section>
</div>
<div class="hidden-content control-hide">
<p style="text-align: center;"><img src="http://p8.itc.cn/q_70/images03/20200922/8fc1b83569bc4502a1cb65597900a81f.png" /></p>
<p style="text-align: left;">Transformer的核心部分，是右边的两个黑色实线框圈起来的两部分，左边是编码器 <span>（Encoder）</span>，右边是解码器 <span>（Decoder）</span>。 </p>
<p style="text-align: left;">可以看见，编/解码器主要由两个模块组合成：前馈神经网络 <span>（图中蓝色的部分）</span>和注意力机制 <span>（图中玫红色的部分）</span>，解码器通常多一个 <span>（交叉）</span>注意力机制。 </p>
<p style="text-align: left;">Transformer最重要的部分，就是注意力机制。</p>
<p style="text-align: left;">通俗来讲，注意力机制在图像处理中的应用，是让机器“像人一样特别注意图像的某个部分”，就像我们在看图时，通常会“特别关注”图中的某些地方。</p>
<p style="text-align: center;"><img src="http://p6.itc.cn/q_70/images03/20200922/51df274109fc4657b678711dcdb5004a.png" /></p>
<p style="text-align: left;">这其中，自注意力机制是定义Transformer模型特征的关键，其中一个重点难题就在于它的时间复杂度和空间复杂度上。</p>
<p style="text-align: left;">由于注意力机制直接将序列 <span>（sequence）</span>两两比较，导致计算量巨大 <span>（计算量变成O(n²)）</span>。 </p>
<p style="text-align: left;">最近，大量论文提出了新的Transformer“变种”，它们的根本目的都是加速模型的效率，但如果一篇篇去看，可能有点眼花缭乱。</p>
<p style="text-align: left;">为此，Google AI的研究人员特意整理了一篇Transformer模型的发展论文，仔细讲解它们的出处。</p>
<p>“变种”后的Transformer模型 2种分类方法 </p>
<p style="text-align: left;">按 <strong>使用方法</strong>来分类的话，Transformer模型可以分成如下3类： </p>
<blockquote>
<p style="text-align: left;">只用编码器：可用于分类 </p>
<p>只用解码器：可用于语言建模 </p>
<p>编码器-解码器：可用于机器翻译 </p>
</blockquote>
<p style="text-align: left;">只用编码器：可用于分类 </p>
<p>只用解码器：可用于语言建模 </p>
<p>编码器-解码器：可用于机器翻译 </p>
<p style="text-align: left;">但如果按这些变种的 <strong>提高效率的原理</strong>，也就是“高效方法”来分类，那么Transformer模型的这些“变种”则可以被分成如下几类： </p>
<p style="text-align: center;"><img src="http://p5.itc.cn/q_70/images03/20200922/c383f47362d44d118db542eda0155d21.png" /></p>
<blockquote>
<p style="text-align: left;">Fixed Patterns（固定模式）：将视野限定为固定的预定义模式，例如局部窗口、固定步幅块，用于简化注意力矩阵；</p>
<p style="text-align: left;">Learnable Patterns（可学习模式）：以数据驱动的方式学习访问模式，关键在于确定token相关性。</p>
<p style="text-align: left;">Memory（内存）：利用可以一次访问多个token的内存模块，例如全局存储器。</p>
<p style="text-align: left;">Low Rank（低秩）：通过利用自注意力矩阵的低秩近似，来提高效率。</p>
<p style="text-align: left;">Kernels（内核）：通过内核化的方式提高效率，其中核是注意力矩阵的近似，可视为低秩方法的一种。</p>
<p style="text-align: left;">Recurrence（递归）：利用递归，连接矩阵分块法中的各个块，最终提高效率。</p>
</blockquote>
<p style="text-align: left;">Fixed Patterns（固定模式）：将视野限定为固定的预定义模式，例如局部窗口、固定步幅块，用于简化注意力矩阵；</p>
<p style="text-align: left;">Learnable Patterns（可学习模式）：以数据驱动的方式学习访问模式，关键在于确定token相关性。</p>
<p style="text-align: left;">Memory（内存）：利用可以一次访问多个token的内存模块，例如全局存储器。</p>
<p style="text-align: left;">Low Rank（低秩）：通过利用自注意力矩阵的低秩近似，来提高效率。</p>
<p style="text-align: left;">Kernels（内核）：通过内核化的方式提高效率，其中核是注意力矩阵的近似，可视为低秩方法的一种。</p>
<p style="text-align: left;">Recurrence（递归）：利用递归，连接矩阵分块法中的各个块，最终提高效率。</p>
<p style="text-align: left;">可以看见，近期Transformer相关的研究都被分在上面的图像中了，非常清晰明了。</p>
<p style="text-align: left;">了解完分类方法后，接下来就是Transformer模型的各种变体了。</p>
<p>17种经典“X-former” </p>
<p style="text-align: left;"><strong>1、Memory Compressed Transformer（2018）</strong></p>
<p style="text-align: left;">这是让Transformer能更好地处理长序列的早期尝试之一，主要修改了两个部分：定位范围注意、内存压缩注意。</p>
<p style="text-align: left;">其中，前者旨在将输入序列分为长度相似的模块，并在每个部分中运行自注意力机制，这样能保证每个部分的注意力成本不变，激活次数就能根据输入长度线性缩放。</p>
<p style="text-align: left;">后者则是采用跨步卷积，减少注意力矩阵的大小、以及注意力的计算量，减少的量取决于跨步的步幅。</p>
<p style="text-align: left;"><strong>2、Image Transformer（2018）</strong></p>
<p style="text-align: left;">这是个受卷积神经网络启发的Transformer变种，重点是局部注意范围，即将接受域限制为局部领域，主要有两种方案：一维局部注意和二维局部注意。</p>
<p style="text-align: left;"><img src="http://p2.itc.cn/q_70/images03/20200922/4c7a7f2566814f299882b5ea5d63ba4a.png" /></p>
<p style="text-align: left;">不过，这种模型有一个限制条件，即要以失去全局接受域为代价，以降低存储和计算成本。</p>
<p style="text-align: left;"><strong>3、 Set Transformer（2019）</strong></p>
<p style="text-align: left;">这个模型是为解决一种特殊应用场景而生的：输入是一组特征，输出是这组特征的函数。</p>
<p style="text-align: left;"><img src="http://p1.itc.cn/q_70/images03/20200922/1686a55666414527aca4b886cf6d1b35.png" /></p>
<p style="text-align: left;">它利用了稀疏高斯过程，将输入集大小的注意复杂度从二次降为线性。</p>
<p style="text-align: left;"><strong>4、Sparse Transformer（2019）</strong></p>
<p style="text-align: left;">这个模型的关键思想，在于仅在一小部分稀疏的数据对上计算注意力，以将密集注意力矩阵简化为稀疏版本。</p>
<p style="text-align: left;">不过这个模型对硬件有所要求，需要自定义GPU内核，且无法直接在TPU等其他硬件上使用。</p>
<p style="text-align: left;"><strong>5、Axial Transformer（2019）</strong></p>
<p style="text-align: center;"><img src="http://p5.itc.cn/q_70/images03/20200922/453e2eb5cc7c41d09ef7e48813e36458.png" /></p>
<p style="text-align: left;">这个模型主要沿输入张量的单轴施加多个注意力，每个注意力都沿特定轴混合信息，从而使沿其他轴的信息保持独立。</p>
<p style="text-align: left;">由于任何单轴的长度通常都比元素总数小得多，因此这个模型可以显着地节省计算和内存。</p>
<p style="text-align: left;"><strong>6、Longformer（2020）</strong></p>
<p style="text-align: left;">Sparse Transformer的变体，通过在注意力模式中留有空隙、增加感受野来实现更好的远程覆盖。</p>
<p style="text-align: left;">在分类任务上，Longformer采用可以访问所有输入序列的全局token（例如CLS token）。</p>
<p style="text-align: left;"><strong>7、Extended Transformer Construction（2020）</strong></p>
<p style="text-align: left;">同样是Sparse Transformer的变体，引入了一种新的全局本地注意力机制，在引入全局token方面与Longformer相似。</p>
<p style="text-align: left;">但由于无法计算因果掩码，ETC不能用于自动回归解码。</p>
<p style="text-align: left;"><strong>8、BigBird（2020）</strong></p>
<p style="text-align: left;">与Longformer一样，同样使用全局内存，但不同的是，它有独特的“内部变压器构造（ITC）”，即全局内存已扩展为在sequence中包含token，而不是简单的参数化内存。</p>
<p style="text-align: left;">然而，与ETC一样，BigBird同样不能用于自动回归解码。</p>
<p style="text-align: left;"><strong>9、Routing Transformer（2020）</strong></p>
<p style="text-align: left;">提出了一种基于聚类的注意力机制，以数据驱动的方式学习注意力稀疏。为了确保集群中的token数量相似，模型会初始化聚类，计算每个token相对于聚类质心的距离。</p>
<p style="text-align: left;"><strong>10、Reformer（2020）</strong></p>
<p style="text-align: left;">一个基于局部敏感哈希 <span>（LSH）</span>的注意力模型，引入了可逆的Transformer层，有助于进一步减少内存占用量。 </p>
<p style="text-align: left;">模型的关键思想，是附近的向量应获得相似的哈希值，而远距离的向量则不应获得相似的哈希值，因此被称为“局部敏感”。</p>
<p style="text-align: left;"><strong>11、Sinkhorn Transformer（2020）</strong></p>
<p style="text-align: left;">这个模型属于分块模型，以分块的方式对输入键和值进行重新排序，并应用基于块的局部注意力机制来学习稀疏模式。</p>
<p style="text-align: left;"><strong>12、Linformer（2020）</strong></p>
<p style="text-align: left;">这是基于低秩的自注意力机制的高效Transformer模型，主要在长度维度上进行低秩投影，在单次转换中按维度混合序列信息。</p>
<p style="text-align: left;"><strong>13、Linear Transformer（2020）</strong></p>
<p style="text-align: left;">这个模型通过使用基于核的自注意力机制、和矩阵产品的关联特性，将自注意力的复杂性从二次降低为线性。</p>
<p style="text-align: left;">目前，它已经被证明可以在基本保持预测性能的情况下，将推理速度提高多达三个数量级。</p>
<p style="text-align: left;"><strong>14、Performer（2020）</strong></p>
<p style="text-align: left;">这个模型利用正交随机特征（ORF），采用近似的方法避免存储和计算注意力矩阵。</p>
<p style="text-align: left;"><strong>15、Synthesizer models（2020）</strong></p>
<p style="text-align: left;">这个模型研究了调节在自注意力机制中的作用，它合成了一个自注意力模块，近似了这个注意权重。</p>
<p style="text-align: left;"><strong>16、Transformer-XL（2020）</strong></p>
<p style="text-align: left;">这个模型使用递归机制链接相邻的部分。基于块的递归可被视为与其他讨论的技术正交的方法，因为它没有明确稀疏密集的自注意力矩阵。</p>
<p style="text-align: left;"><strong>17、Compressive Transformers（2020）</strong></p>
<p style="text-align: left;">这个模型是Transformer-XL的扩展，但不同于Transformer-XL，后者在跨段移动时会丢弃过去的激活，而它的关键思想则是保持对过去段激活的细粒度记忆。</p>
<p style="text-align: left;">整体来说，这些经典模型的参数量如下：</p>
<p style="text-align: center;"><img src="http://p4.itc.cn/q_70/images03/20200922/30cb09ca191a4ffaa27b844026355232.png" /></p>
<p style="text-align: left;">更详细的解读 <span>（包括具体的模型参数等）</span>，以及对Transformer未来趋势的预测，可以戳下方传送门查看整篇论文。 </p>
<p>作者介绍 </p>
<p style="text-align: center;"><img src="http://p8.itc.cn/q_70/images03/20200922/8a73a219b4bf422db1ede20c9661cf4e.png" /></p>
<p style="text-align: left;">论文一作Yi Tay，硕士和博士均毕业于新加坡国立大学计算机科学。</p>
<p style="text-align: left;">目前，Yi Tay在Google AI从事研究工作，主要方向是自然语言处理和机器学习。</p>
<p style="text-align: left;"><strong>传送门</strong></p>
<p style="text-align: left;"><span style="font-size: 16px;"><span style="font-size: 16px;">论文链接：</span></span></p>
<p><span style="font-size: 16px;">https://www.arxiv-vanity.com/papers/2009.06732</span></p>
<p>— <strong>完</strong>— </p>
<p><span>本文系网易新闻•网易号特色内容激励计划签约账号【量子位】原创内容，未经账号授权，禁止随意转载。</span><span style="font-size: 16px;"></span></p>
<p><strong>好课推荐 | 0基础学Python</strong></p>
<p><span style="font-size: 16px;">《动手学Python》课程由</span><strong><span style="font-size: 16px;">上海交大博士教研团队</span></strong><span style="font-size: 16px;">研发，在线学习平台由上海</span><strong><span style="font-size: 16px;">交大AI实验室</span></strong><span style="font-size: 16px;">技术支持，旷视、文远知行、图森未来、来也科技等</span><strong><span style="font-size: 16px;">知名AI</span></strong><span style="font-size: 16px;"><strong>企业CEO/CTO力荐</strong></span><span style="font-size: 16px;">。</span></p>
<p style="text-align: left;"><span style="font-size: 16px;">0基础可入门，限时开放体验课！扫码即刻开启Python学习之旅：</span></p>
<p><span style="font-size: 16px;"><strong>量子位 </strong></span><span style="font-size: 16px;">QbitAI · 头条号签约作者</span></p>
<p>վ'ᴗ' ի 追踪AI技术和产品新动态</p>
<p><span>一键三连「分享」、「点赞」和「在看」</span></p>
<p><span>科技前沿进展日日相见~</span></p>