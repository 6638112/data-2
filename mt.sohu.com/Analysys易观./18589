➜相宜本草于贵洋：国民美妆品牌的数字化升级之路
http://www.sohu.com/a/433726581_115326	26475
<p><span style="font-size: 16px;">近几年，<strong>美妆行业驶进了增长快车道，民族企业势如破竹</strong>，取得了前所未有的成就。曾经的零售只分前端和后台，随着人工智能等前沿科技的高速发展，传统的零售模式受到冲击，企业必须因时制宜，<strong>美妆智能数字化革命的风潮正在开启</strong>，美妆行业正在不断探索企业与数字化的深度融合，开创新的商业模式。</span> </p>
<p><span style="font-size: 16px;"> </span><span style="font-size: 16px;">虽然美妆行业进行数字化转型是大势所趋，但是数字化无章可循，<strong>企业应如何结合自身发展阶段、规模找到适合的突破点？如何把握好营销的力度？如何制定高效的运营策略？</strong></span><span style="font-size: 16px;"> </span><span style="font-size: 16px;">2020 年 10 月 30 日，易观 A10 数据智能峰会上海站圆满落幕。作为本次峰会的特邀嘉宾，</span><span style="font-size: 16px;"><strong>相宜本草大数据负责人于贵洋先生</strong></span><span style="font-size: 16px;">，在峰会现场以相宜本草的数字化转型架构搭建为切入点，阐述精准的数字化运营和客户触达是如何推动企业数据升级的，为美妆行业的数字化转型添薪加火。</span></p>
<p><img src="http://p6.itc.cn/images01/20201123/c4605235a9a340b88294d6e70ac07a1e.jpeg" max-width="600" /></p>
<h1><span style="font-size: 16px;"><strong><span style="font-size: 16px;">相宜本草的品牌文化</span></strong><p><strong></strong><span style="font-size: 16px;"><strong><span style="font-size: 16px;">淡妆浓抹总相宜</span></strong></span></p></span></h1>
<p><strong></strong><span style="font-size: 16px;"><strong><span style="font-size: 16px;">淡妆浓抹总相宜</span></strong></span></p>
<p><span style="font-size: 16px;">于贵洋先生表示，相宜本草的品牌文化是追求和谐、健康之美。相宜本草一直以来精研中草药护肤的功效，传承中国传统的中医药文化。近年来，相宜本草已经陆续在云南、甘肃、川藏等地设立种植基地，研发出多种中草药护肤产品。</span><span style="font-size: 16px;"> </span><span style="font-size: 16px;"> </span></p>
<p><img src="http://p3.itc.cn/images01/20201123/ddfd23a4c369455095239dd6c6493538.png" max-width="600"> <span style="font-size: 16px;"> </span> </img></p>            <div class="lookall-box">
<div class="lookall-shadow"></div>
<section class="lookall">
<a href="javascript:;" class="show-all" id="showMore">
<em>展开全文</em>
</a>
</section>
</div>
<div class="hidden-content control-hide">
<h1><span style="font-size: 16px;"><strong><span style="font-size: 16px;">【重构】：1到1.1的过程</span></strong></span></h1>
<h1><span style="font-size: 16px;"><strong><span style="font-size: 16px;">与易观数科的相识</span></strong></span></h1>
<h1><span style="font-size: 16px;"><strong><span style="font-size: 16px;">数据升级之路</span></strong></span></h1>
<p><span style="font-size: 16px;"> </span> <span style="font-size: 16px;"> </span> <img src="http://p6.itc.cn/images01/20201123/f5742e0e0bcb4d27bb3862885b45bc5c.png" max-width="600"> <span style="font-size: 16px;"> </span> </img></p>
<p><span style="font-size: 16px;"> </span> <strong><span style="font-size: 16px;"><span style="font-size: 16px;">▌</span>用户分群</span></strong> <span style="font-size: 16px;">在疫情期间，虽然许多线下门店都已经关闭，但门店的美容顾问工作并没有因此停滞。于贵洋先生表示，门店的美容顾问都有自己的私人客户，她们会通过线上平台进行产品推广和用户触达。通过企业内部CRM分层，不同层级的员工对不同类型的用户进行触达，再分别对用户进行路径分析，提出优化策略。</span> </p>
<p> <span style="font-size: 16px;">在最后，于贵洋先生指出，<strong>进行用户渠道、用户路径、用户画像等数据的分析对一些企业而言是困难的，此时就需要借助供应商的力量，依托供应商的技术和能力来推动企业的数字化升级。</strong></span></p>
➜腾讯看点王展雄：实时数仓与多维实时分析系统搭建
http://www.sohu.com/a/433754122_115326	33491
<p><span style="font-size: 16px;">近几年，数字驱动的口号越喊越响，在这样一个用数据说话的时代，</span><span style="font-size: 16px;"><strong>数据在一定程度上决定企业的业务和决策</strong></span><span style="font-size: 16px;">。而从数据驱动的方面考虑，多维实时数据分析系统的重要性也不言而喻。但是当数据量巨大的情况下，企业对数据技术也提出了更高的要求，但是要实现极低延迟的实时计算和亚秒级的多维实时查询还是有难度的。</span></p>
<p><span style="font-size: 16px;"> </span><span style="font-size: 16px;">以腾讯看点来说，一天上报的数据量达到万亿级的规模，企业</span><span style="font-size: 16px;">要</span><strong><span style="font-size: 16px;">如何知道在不同人群的推荐效果怎么样？在不同地域中最火的内容分别是什么？被举报最多的内容和账号是哪些？在某个时间段内有多少用户消费了内容？</span></strong><span style="font-size: 16px;"></span><span style="font-size: 16px;"> </span><span style="font-size: 16px;">以下内容根据其演讲整理：</span></p>
<h1><span style="font-size: 16px;"><strong><span style="font-size: 16px;">01调研</span></strong></span></h1>
<p><span style="font-size: 16px;"></span><span style="font-size: 16px;">在分享一开始，王展雄首先总结了多维实时数据分析系统可以解决的主要痛点问题，比如：</span><span style="font-size: 16px;"> </span></p>
<ul>
<li><strong><span style="font-size: 16px;">推荐同学10分钟前上了一个推荐策略，想知道在不同人群的推荐效果怎么样？</span></strong></li>
<li><strong><span style="font-size: 16px;">运营同学想知道，在广东省的用户中，最火的广东地域内容是哪些？</span></strong></li>
<li><strong><span style="font-size: 16px;">审核同学想知道，过去5分钟，游戏类被举报最多的内容和账号是哪些？</span></strong></li>
<li><strong><span style="font-size: 16px;">老板可能想了解，过去10分钟有多少用户在看点消费了内容？</span></strong><span style="font-size: 16px;"></span></li>
<li><strong><span style="font-size: 16px;"></span></strong></li>
</ul>
<p><span style="font-size: 16px;">首先，C端的行为数据上报过来需要经过Spark多层离线计算，把最终结果初步过ES，初步之后再提供离线数据平台进行查询。这个过程延时最少在3-6小时，目前最常见的就是隔天的查询。加之腾讯看点的数据量大，稳定性较弱，离线分析平台难以满足多样化的需求。</span> <span style="font-size: 16px;"> </span> </p>
<p><span style="font-size: 16px;">此外，企业内部现有的准实时数据查询平台的底层技术用的是Kudu+Impala，Impala虽然是MPP架构的大数据计算引擎，并且访问以列式存储数据的Kudu。但是Kudu+Impala这种通用大数据处理框架只是相比Spark+Hdfs这种离线分析框架而言具有速度优势，对于实时数据分析场景来说，查询响应的速度和数据的延迟仍然比较高，无法满足实时性要求更高的场景，无法提供良好的交互式用户体验。</span> </p>
<h1><span style="font-size: 16px;"><strong><span style="font-size: 16px;">02了解项目背景</span></strong></span></h1>
<p><span style="font-size: 16px;">技术研发是要根据企业的业务需求进行的，腾讯看点内容业务的运行模式是作者发布的内容被内容中心引入，经过内容审核链路，审核结果启用或者下架。启用的内容给到推荐系统和运营系统，然后推荐系统和运营系统将内容通过手机QQ、微信、QQ浏览器，还有一些独立端的APP进行C侧分发。内容分发给C侧用户之后，用户会产生各种行为，曝光、点击、举报等，通过埋点上报实时接入到消息队列中。</span></p>
<p><span style="font-size: 16px;"> </span> <img src="http://p4.itc.cn/images01/20201123/7d6b00c8ea1f496da535a36aeef1a728.png" max-width="600"> </img></p>
<p><span style="font-size: 16px;">实时数据仓库能够按照信息流的业务场景进行内部维度关联、用户画像关联，聚合各种内容，提高下游使用实时数据的便捷性。多维实时数据分析系统可以实时分析系统消费了上一轮数据实时数据仓库，然后利用了OLAP存储计算引擎，让海量的数据进行了高效的存储，提供高性能的查询。</span> <span style="font-size: 16px;"> </span> </p>
<h1><span style="font-size: 16px;"><strong><span style="font-size: 16px;">03方案选型</span></strong></span></h1>
<p><span style="font-size: 16px;">每种方案都有不足，腾讯看点对比行业内的领先方案，最终选择</span><span style="font-size: 16px;"><strong><span style="font-size: 16px;">最符合企业业务场景的方案</span></strong></span><span style="font-size: 16px;">。</span></p>            <div class="lookall-box">
<div class="lookall-shadow"></div>
<section class="lookall">
<a href="javascript:;" class="show-all" id="showMore">
<em>展开全文</em>
</a>
</section>
</div>
<div class="hidden-content control-hide">
<ul>
<li><span style="font-size: 16px;">实时数仓的选型：腾讯看点选择了业界比较成熟的</span><span style="font-size: 16px;"><strong><span style="font-size: 16px;">Lambda架构</span></strong></span><span style="font-size: 16px;">，Lambda架构具有灵活性高、容错性高、成熟度高和迁移成本低众多有点；缺点是实时、离线数据需要使用两套代码，可能业务逻辑修改了，但是批次没有跟上。腾讯看点对这个问题的处理方法是每天都进行数据对账工作，如果有异常则进行告警。</span></li>
</ul>
<ul>
<li><span style="font-size: 16px;">实时计算引擎选型：</span><span style="font-size: 16px;"><strong>选择了Flink作为实时计算引擎</strong></span><span style="font-size: 16px;">。因为Flink设计之初就是为了流处理，此外，Flink还具有Exactly-once的准确性、轻量级Checkpoint容错机制、低延时高吞吐和易用性高的特点，是最佳选择。</span></li>
</ul>
<ul>
<li><span style="font-size: 16px;">实时存储引擎选型：腾讯看点的业务对维度索引、支持高并发、预聚合、高性能实时多维OLAP查询有要求，而Hbase、Tdsql和ES都不能满足。Druid存在按照时序划分Segment的缺陷，无法将同一个内容，存放在同一个Segment上，计算全局TopN只能是近似值。综合对比，最终</span><span style="font-size: 16px;"><strong><span style="font-size: 16px;">选择了MPP数据库引擎ClickHouse</span></strong><span style="font-size: 16px;">。</span></span></li>
</ul>
<h1></h1>
<h1></h1>
<h1></h1>
<h1><span style="font-size: 16px;"><strong><span style="font-size: 16px;">04设计目标与设计难点分析</span></strong></span></h1>
<p><span style="font-size: 16px;"> </span> <img src="http://p3.itc.cn/images01/20201123/fdb17d13abb244b6b6d50eef8b3613ad.png" max-width="600"> <span style="font-size: 16px;"> </span> </img></p>
<h1><span style="font-size: 16px;"><strong><span style="font-size: 16px;">05难点攻克</span></strong></span></h1>
<p><span style="font-size: 16px;"> </span><img src="http://p0.itc.cn/images01/20201123/343f48ef56f8404fab4f0b6882b95e00.png" max-width="600"><span style="font-size: 16px;"> </span></img></p>
<ul>
<li>接入层主要是从千万级/s的原始消息队列中，拆分出不同行为数据的微队列，拿看点的视频来说，拆分过后，数据就只有百万级/s了；</li>
<li>实时计算层主要负责，多行行为流水数据进行行转列，实时关联用户画像数据和内容维度数据；</li>
<li><span style="font-size: 16px;">实时数仓存储层主要是设计出符合看点业务的，下游好用的实时消息队列。我们暂时提供了两个消息队列，作为实时数仓的两层。一层DWM层是内容ID-用户ID粒度聚合的，就是一条数据包含内容ID-用户ID还有B侧内容数据、C侧用户数据和用户画像数据；另一层是DWS层，是内容ID粒度聚合的，一条数据包含内容ID，B侧数据和C侧数据。可以看到内容ID-用户ID粒度的消息队列流量进一步减小到十万级/s，内容ID粒度的更是万级/s，并且格式更加清晰，维度信息更加丰富。</span></li>
<li><span style="font-size: 16px;"></span></li>
</ul>
<ul>
<li>实时写入层主要是负责Hash路由将数据写入；</li>
<li>OLAP存储层利用MPP存储引擎，设计符合业务的索引和物化视图，高效存储海量数据；</li>
<li>后台接口层提供高效的多维实时查询接口。</li>
</ul>
<h1><span style="font-size: 16px;"><strong>实时计算</strong></span></h1>
<p><span style="font-size: 16px;"><span style="font-size: 16px;">实</span><span style="font-size: 16px;">时计算分为实时关联和实时数仓。</span></span></p>
<ul>
<li><span style="font-size: 16px;">第一个是，在Flink实时计算环节，</span><span style="font-size: 16px;"><strong>先按照1分钟进行了窗口聚合，将窗口内多行行为数据转一行多列的数据格式</strong></span><span style="font-size: 16px;">，经过这一步操作，原本小时级的关联耗时下降到了十几分钟，但是还是不够的。</span></li>
</ul>
<p><span style="font-size: 16px;"> </span><img src="http://p5.itc.cn/images01/20201123/e547bd18cfb84937805bf18073c1a742.png" max-width="600"><span style="font-size: 16px;"> </span></img></p>
<ul>
<li>第二个是，在访问HBase内容之前设置一层Redis缓存，因为1000条数据访问HBase是秒级的，而访问Redis是毫秒级的，访问Redis的速度基本是访问HBase的1000倍。为了防止过期的数据浪费缓存，缓存过期时间设置成24小时，同时通过监听写HBase Proxy来保证缓存的一致性，并将访问时间从十几分钟变成了秒级。</li>
<li>第三个是，上报过程中会上报不少非常规内容ID，这些内容ID在内容HBase中是不存储的，会造成缓存穿透的问题。所以在实时计算的时候，系统直接过滤掉这些内容ID，防止缓存穿透，又减少一些时间。</li>
<li><span style="font-size: 16px;">第四个是，因为设置了定时缓存，会引入一个缓存雪崩的问题。为了防止雪崩，我们</span><span style="font-size: 16px;"><strong>在实时计算中，进行了削峰填谷的操作，错开设置缓存的时间</strong></span><span style="font-size: 16px;">。</span></li>
</ul>
<p><span style="font-size: 16px;">而实时数仓的难度在于，它处于比较新的领域，并且各个公司各个业务差距比较大，要设计出方便，好用，符合看点业务场景的实时数仓是有难度的。</span> </p>
<p><span style="font-size: 16px;">在没有数仓的时候，需要消费千万级/s的原始队列，进行复杂的数据清洗，然后再进行用户画像关联、内容维度关联，才能拿到符合要求格式的实时数据，开发和扩展的成本都会比较高，如果想开发一个新的应用，又要走一遍这个流程。有了数仓之后，如果想开发内容ID粒度的实时应用，就直接申请TPS万级/s的DWS层的消息队列。开发成本变低很多，资源消耗小很多，可扩展性也强很多。</span> <span style="font-size: 16px;"> </span> </p>
<p><span style="font-size: 16px;">开发腾讯看点系统的实时数据大屏，原本需要进行如上所有操作，才能拿到数据。现在只需要消费DWS层消息队列，写一条Flink SQL即可，仅消耗2个cpu核心，1G内存。可以看到，以50个消费者为例，建立实时数仓前后，下游开发一个实时应用，可以减少98%的资源消耗。包括计算资源，存储资源，人力成本和开发人员学习接入成本等等。并且消费者越多，节省越多。就拿Redis存储这一部分来说，一个月就能省下上百万人民币。</span> </p>
<h1><span style="font-size: 16px;"><strong>实时储存</strong></span></h1>
<ul>
<li><span style="font-size: 16px;"><strong><span style="font-size: 16px;">分布式-高可用</span></strong></span></li>
</ul>
<ul>
<li><span style="font-size: 16px;"><strong><span style="font-size: 16px;">海量数据-写入</span></strong></span></li>
</ul>
<ul>
<li><span style="font-size: 16px;"><strong><span style="font-size: 16px;">高性能-查询</span></strong></span></li>
</ul>
<p><span style="font-size: 16px;"> </span><span style="font-size: 16px;"> </span><img src="http://p5.itc.cn/images01/20201123/d742d5ae010d4633877493e9c367cef7.png" max-width="600"><span style="font-size: 16px;"> </span></img></p>
<p> <span style="font-size: 16px;">分享最后，王展雄指出，腾讯看点的实时分析系统力亚秒级的响应查询请求，在未命中缓存情况下，<strong>过去30分钟的查询，99%的请求耗时在1秒内；过去24小时的查询，90%的请求耗时在5秒内，99%的请求耗时在10秒内</strong>。</span></p>