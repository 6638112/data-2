➜自动驾驶事故，到底该由谁担责？
http://www.sohu.com/a/422698778_115207	5760
<p><img src="http://p3.itc.cn/q_70/images03/20201005/348b128b01e34c819d7e15023b88a19b.jpeg" /></p>
<p>还记得2年多前Uber的那起自动驾驶汽车导致行人死亡的车祸吗？最新消息是，今年的9月15日，亚利桑那州的大陪审团决定以过失杀人罪起诉当时Uber自动驾驶汽车前安全驾驶员拉斐尔·瓦斯奎兹，并建议判处其2.5年有期徒刑，而这位安全员当庭表示拒绝认罪，这场官司可能还要继续打下去。</p>
<p><img src="http://p8.itc.cn/q_70/images03/20201005/cf1c1fac353343aeb77aaeb62e71561b.png" /></p>
<p>这场事故背后的责任方Uber呢？其实早在去年3月份，作为肇事方的Uber已经被美国法院判定无罪。</p>
<p>被称为“自动驾驶致行人死亡第一案”的当事方就这样轻而易举地逃脱担责，让这个确实存在一定过错的安全员来承担全部罪责。这结果确实令人唏嘘，难道这是一起“大公司作恶，小职员背锅”的司法腐败？抑或是，这场判决是美国司法的老模式遇到新问题，不知道如何对自动驾驶算法系统及其所有者做出裁定？</p>
<p>在有安全员监控的自动驾驶测试或者商用中，我们自然还是会把车辆的安全事故责任归咎于这个安全员，但一旦真正的无人驾驶大规模普及，车上的安全员，甚至是方向盘、刹车都去掉之后，车辆的安全事故责任，那自然就要算到研发和使用这套自动驾驶算法的企业主体身上了。那么到时候，关于自动驾驶算法的追责将变得更加复杂。</p>
<p>在讨论这一问题之前，我们不妨回到Uber的这起车祸细节中，来看下这场车祸判决存在哪些争议点，Uber是否真的可以全身而退？一旦去掉安全员，无人汽车和自动驾驶算法该如何担责？这些看似未来才会遭遇的问题已经摆到了你我的面前，亟待思考和讨论。</p>            <div class="lookall-box">
<div class="lookall-shadow"></div>
<section class="lookall">
<a href="javascript:;" class="show-all" id="showMore">
<em>展开全文</em>
</a>
</section>
</div>
<div class="hidden-content control-hide">
<p>回到现场：车祸是如何发生的？</p>
<p>去年11月，美国国家安全运输委员会（NTSB）发布了一份报告，披露了Uber自动驾驶汽车在碰撞前10秒的细节。值得注意的是，当时已经判决了Uber平台无责，但是这份报告中却指出了Uber自动驾驶系统有种种漏洞。</p>
<p>这起车祸的大致经过是这样。2018年3月18日晚上，亚利桑那州坦佩市一位女性在推着自行车过马路时，被时速60多公里Uber无人驾驶汽车撞死。</p>
<p>如果这辆车只是一辆普通车辆，那么事故责任就很明显，一边是行人横穿马路，负有一定责任，但车辆司机因没有及时刹车和避让，要负主要责任。但这辆车是Uber的无人驾驶测试车辆，车上面配有一名安全员，负责处理车辆的紧急情况。</p>
<p>在这起车祸中，这名安全员显然没有尽职尽责。根据调查，这名安全员在行车过程中，一直在通过手机观看类似于“中国好声音”的娱乐节目，这期间监控摄像头拍到他一直在反复低头，直到事故发生前的0.5秒，他才注意到车辆前方的行人，最后只是在撞到后的0.7秒才踩下刹车，但事故已经发生了。</p>
<p><img src="http://p4.itc.cn/q_70/images03/20201005/d356fb6242ea44b2b068462270dfae63.png" /></p>
<p>这场事故判决安全员担责是毫无问题的。毕竟他的职责就是确保车辆行驶安全和道路行人安全，可由于他的疏忽大意，直接造成了这一严重事故。日常生活中，大量的行车事故大多由这类疏忽大意造成。</p>
<p>但正是Uber无人驾驶车辆的自动驾驶系统给了安全员一种错觉，认为车辆可以自行判断前方的路况，而自己可以偷懒去看手机。这也是自动驾驶技术等级中L3级别的困境。车辆可以高度自动驾驶，但是出了事故要算驾驶员的。那么怎么可以让驾驶员放心的休息或者娱乐游戏呢？</p>
<p>回到Uber这辆车，难道它就没有任何问题么？从调查来看，问题也很多。</p>
<p>在Uber车辆撞到行人前的10秒中，车辆本来识别到这个行人并避免车祸的。但是一系列系统的误判导致了车辆未曾减速就撞了上去。报告中有几个关键数据：在9.9秒到5.8秒中，汽车从56公里加速到70公里；在5.6秒，汽车毫米波雷达（Radar）第一次检测到前方有物体，并识别其为“汽车”，5.2秒，汽车激光雷达（Lidar）第一次检测到前方物体，将其识别为“其他”，判定其静止不动。4.2秒到2.7秒，汽车对识别对象在“汽车”和“未知”之间来回摇摆，但是没有参考对物体的跟踪历史记录，最终将其判定为静止物体。</p>
<p><img src="http://p1.itc.cn/q_70/images03/20201005/ff9122a64a1642349a6eb2faffc3cfc8.png" /></p>
<p>2.6秒到1.2秒的时间，激光雷达才将物体识别为静止的自行车，但又出现判定摇摆，等到重新识别为自动车，并决定制动。但车辆真正制动是在车祸前的0.2秒开始。这时时速64公里的车辆已经无法避免撞到行人。车祸发生。</p>
<p>我们看到，在车祸发生前的几秒钟，车辆发生了多次摇摆不定的误判，浪费了大量时间。根据NTSB报告指出，造成事故的关键问题就是，软件无法正确预测受害者的类别和运动轨迹。如果系统及早正确地识别出前方物体是行人，就应该大幅放慢速度，或者设法绕开避让。</p>
<p>但是Uber的自动驾驶系统并没有如此谨慎行事，反而是因为Uber认为紧急制动系统会造成车辆的不稳定，所以对该系统做了限制。</p>
<p>也就是说，Uber把自动驾驶系统的刹车当成了最后才考虑的因素，真是细思极恐。</p>
<p>自动驾驶系统开车，安全员负责？</p>
<p>如果按照NTSB的调查，那么Uber的自动驾驶系统就存在巨大安全缺陷，首先是汽车的识别算法的准确度和时效性问题，其次就是对于紧急制动系统的设置权限问题。NTSB得出结论说，Uber取消车辆出厂自带的自动紧急制动系统的做法，增加了在公共道路上测试自动驾驶车辆的风险。</p>
<p><img src="http://p2.itc.cn/q_70/images03/20201005/5c30b39e950d41e887571b224bf59ee3.png" /></p>
<p>据调查报告，这辆Uber汽车在车祸前，已经以自动驾驶模式运行了约19分钟，车辆大约至少行驶了约22公里。那么在这段距离内，如果安全员没有踩过一次刹车，那就意味着自动驾驶系统也很可能没有启动过一次刹车。如果有开车经验的人来说，哪怕是夜深人静的街道，很少会在以每小时接近70公里的时速下行驶20多公里，都不需要减速或刹车。</p>
<p>如果是Uber真的把刹车权限交给了安全员，那么这个安全员怎么又可能在完全不顾及自己和行人安全的情况下，在高速行驶中还敢沉浸在娱乐节目当中。</p>
<p>也就是说，Uber将刹车权限交给安全员的同时，却没有让安全员意识到自己要百分百了解这一安全措施。Uber通过设置安全员规避了法律风险，但是它自身却没有预计到车辆的安全风险，也没有尽到告知义务，使得一个被算法“忽悠”的人类成为自动驾驶技术走进现实世界的注脚。</p>
<p>反过来说，一辆汽车的自动驾驶系统在自动驾驶模式下系统没有刹车权限，而是完全需要安全员操作的话，那么这场自动驾驶测试到底意味着什么，一场假的自动驾驶测试吗？</p>
<p>根据Uber的一名离职工程师的说法，“Uber的车祸发生率会还是太高了”，“如果是Waymo出现这样的表现，就会停止测试以找出原因，而Uber则会忽略这一问题”。</p>
<p>这些问题也正是外界诟病Uber的无人驾驶计划的地方。Uber既想通过激进的自动驾驶计划来推进其自动驾驶出租车业务的商业化，又想通过设置安全员来规避其在自动驾驶系统上的缺陷和漏洞，最终出现问题，还可以把责任推给这些雇员。</p>
<p>显然，Uber做到了。在2018年底，Uber又恢复了部分城市的无人车路测，为每辆车配备了2名安全员，并进行更为严格的监控，以及对自动驾驶系统做了优化。</p>
<p>而对于当地的司法机关来说，判决Uber无需担责的原因则很简单，就是“没有任何判决依据”。</p>
<p>无人驾驶之后，谁来真正担责？</p>
<p>因为缺乏法律责任的认定，这次Uber得以“侥幸”逃脱。但根据以上分析，Uber在事实责任面前是难辞其咎的。</p>
<p>首先，Uber自动驾驶系统并没有以安全作为第一考虑要素，而是更强调系统的稳定性和持续性。这是为Uber无人车出现众多安全事故埋下了隐患。如果未来Uber是以这样一套“激进”的算法来推进其无人驾驶出租车的行驶策略，那么，很容易出现车辆以快速行驶优先而忽略道路安全的情况。</p>
<p>其次，该驾驶系统的测试存在纰漏，按照其对制动系统的设置，需要安全员的干预才能完成，这显然是背离自动驾驶技术的本意。显然，这样的系统是无法真正实现无人驾驶的商用的。</p>
<p>就在Uber出现致命事故的同一年，美国的加州却进一步放松了无人驾驶的监管，可以允许车辆上没有安全驾驶员，只需要保证自动驾驶车辆出问题时，能被远程接管即可。</p>
<p>2019年，Waymo就拿到了加州机动车辆管理局（DMV）颁发的完全自动驾驶测试牌照，测试时可以不用安全员。后面在无人驾驶出租车上，乘客也已经可以打到没有安全员的出租车，只是在遇到突发危险后，可以在行驶中按下汽车帮助按钮或在应用程序中与安全员取得联系。</p>
<p><img src="http://p9.itc.cn/q_70/images03/20201005/513ca2c59ecc490b8dce5fe7f31d04e8.png" /></p>
<p>那么，这一情况下，就必须要考虑到无人车的新的责任归属和相关问题了，毕竟车辆出现事故不能再归咎于远程指导的安全员了。</p>
<p>那责任归属其实就比较简单了。在根据正常的交通事故责任认定后，如果排除了对方责任之后，那么事故责任就会判定为无人驾驶汽车的责任，但至于是车生产商、自动驾驶系统提供商或业务运营方来承担责任，则需要根据商业模式的责任划分和对现场事故的原因判定来进行划分。</p>
<p>但这里会有一个法律责任主体缺失问题。在现有一般情况下，几乎每一场事故都会有专门的人来对此负责，大多数都是违规肇事司机，但一旦换成无人驾驶汽车，那么也就找不到这样一个法律责任主体。因为不可能去控告一个购买了无人驾驶汽车的车主吧，毕竟他没有开车，也不可能去控告设计了这个自动驾驶算法的工程师吧，工程师又不是一个人，事故原因也不能仅仅归因于某行代码。那么，归结于提供自动驾驶系统的公司吗？那这样没有任何一家公司会在愿意承担如此巨大的风险了。</p>
<p>也许未来将会有一个由自动驾驶汽车各方和保险公司共同成立的责任主体，这些制造、设计和运营各方根据责任大小承担相应比例的保险费，无人驾驶的私家车主（估计会很少是个人）也会在购买服务中支付一定的保险费用，形成一个保险资产池，来应对可能出现的事故。</p>
<p>这个责任主体对事故承担整体的责任认定和赔偿，同时也在内部形成一套AI测算系统，根据不同汽车厂商的车辆损坏情况、不同自动驾驶算法的事故率和运营商的运营策略来认定具体责任，以决定不同主体未来的保费。</p>
<p>比如，有些汽车厂商以保障车内乘客的安全优先，那么在出现事故导致行人受损后，基于这种策略的公司就要多交保费；如果有些厂商是以保障行人乘客的安全优先，出现车内乘客受伤或致命事故，就要多支付费用，多赔付车内乘客。</p>
<p>可以预见，当自动驾驶无人车普及之后，各种各样复杂状况的责任认定案例会层出不穷。我们必须在此之前就要开始思考和尝试立法工作。而不是等到事情发生之后，才开始摸索。千万不要像Uber案例一样，最终只能把罪责扣在这个不负责任的人类身上，而对自动驾驶算法系统束手无策。</p>
<p>对无人驾驶汽车的严苛管制，并不意味着我们不看好这一产业。在我看来，无人驾驶汽车的前途是非常光明的。尽管会出现这样那样的极端事故，但是无人驾驶在未来一定会比现有的人类驾驶的出行状况是更安全的。</p>
<p><img src="http://p3.itc.cn/q_70/images03/20201005/a75b1d71385148bdbf3d3dbc38d29f80.png" /></p>
<p>就像目前Waymo出现的众多事故中，绝大多数都是人类司机的全责。当未来一旦无人驾驶汽车占据多数的时候，我们就不必再小心这些车辆，而是要更小心人类司机的车辆了。因为自动驾驶系统开车时是不会去看“达人秀”的。</p>
➜为AI而生的IPU芯片，或挑战GPU的霸主位？
http://www.sohu.com/a/422747369_115207	26700
<p><img src="http://p7.itc.cn/q_70/images03/20201005/a182aeaefd1349078036f0a563f3b3a2.jpeg" /></p>
<p>在CPU芯片领域，延续至今的“摩尔定律”正在随着制程工艺逼近物理极限而有了延缓的趋势，甚至失效的可能。就在摩尔定律的增长放缓脚步的同时，半导体芯片的计算也正在从通用走向专用，其中AI计算正是其中增长最快的一种专用计算。</p>
<p>现在，AI计算正在接棒摩尔定律，延续并超越其倍增神话。2019年，OpenAI发布了AI算力的增长情况，结果显示AI算力以3.4个月的倍增时间实现了指数增长，从2012年起，该指标已经增长了30万倍。</p>
<p>在AI算力爆炸式增长的过程中，英伟达的GPU功不可没。广为人知的一个故事就是2012年，来自多伦多大学的Alex和他的团队设计了AlexNet的深度学习算法，并用了2个英伟达的GTX580 GPU进行训练后，打败了其他所有计算机视觉团队开发的算法，成为那一届ImageNet的冠军。</p>
<p>此后，在计算机视觉和自然语言处理领域，GPU的高并行计算能力得到了充分的发挥，英伟达的GPU也随着AI第三次浪潮的崛起而迎来井喷发展。与此同时，更多为机器学习而专门定制的专用芯片开始出现，比如专用集成电路（ASIC）的张量处理单元TPU、神经网络单元NPU以及半定制芯片FPGA等等。</p>
<p>2018年底，英国一家名为Graphcore的创业公司推出了一种专门用于AI计算的处理器芯片IPU（Intelligence Processing Unit）。一经问世，IPU就受到AI界越来越多的关注。</p>
<p><img src="http://p1.itc.cn/q_70/images03/20201005/3a6473f28ce84b198eff82f175aecef0.png" /></p>            <div class="lookall-box">
<div class="lookall-shadow"></div>
<section class="lookall">
<a href="javascript:;" class="show-all" id="showMore">
<em>展开全文</em>
</a>
</section>
</div>
<div class="hidden-content control-hide">
<p>ARM创始人，被称为英国半导体之父的赫曼·豪瑟曾为Graphcore的IPU给出很高评价，将其誉为<strong>“计算机史上三次革命中，继CPU和GPU之后的第三次革命”</strong>。赫曼在芯片产业的地位自然不容置疑，但由于Graphcore是英国芯片产业中为数不多的新生力量，难免赫曼有“护犊子”的打广告之嫌。</p>
<p>IPU出道2年时间，现已推出了量产第二代型号为GC2的IPU。那么，IPU的表现如何，与GPU相比有哪些优势之处，这是本文要重点探讨的问题。</p>
<p>GPU所开启的深度学习</p>
<p>一个广为人们熟知的例子就是，在计算机视觉发展初期的2011年，谷歌大脑想要在YouTube的视频中识别人类和猫，当时这样一个简单的任务，谷歌要动用一家大型数据中心内的 2,000 颗服务器 CPU，这些CPU的运行会产生大量的热量和能耗，关键是代价不菲，很少能有研究人员可以用得起这种规模的服务器。</p>
<p><img src="http://p1.itc.cn/q_70/images03/20201005/749eb44c2d14431ebfe9a48153da7845.png" /></p>
<p>不过在当时，研究人员注意到了英伟达的GPU，斯坦福大学的吴恩达团队开始和英伟达合作，将GPU应用于深度学习。后来证明，只需要12颗英伟达GPU就可以达到相当于2000颗CPU提供的深度学习性能。此后越来越多的AI研究人员开始在GPU上加速其深度神经网络 (DNN)的训练。</p>
<p><img src="http://p0.itc.cn/q_70/images03/20201005/5fe64bff5f3a411b80180db424318e15.png" /></p>
<p>现在我们都知道，GPU能够在深度学习的训练中大显身手，正是源于GPU的计算架构正好适用于深度学习的计算模式。深度学习是一种全新的计算模式，其采用的DNN算法包含数十亿个网络神经元和数万亿个连接来进行并行训练，并从实例中自己学习规律。</p>
<p><strong>深度学习算法主要依赖的基本运算方法有矩阵相称和卷积浮点运算，而GPU多核架构在原本图像渲染中可以大规模处理矩阵乘法运算和浮点运算，很好地可以处理并行计算任务，使得DNN训练速度大幅提升。</strong></p>
<p>此后，GPU成为辅助完成深度学习算法的主流计算工具，大放异彩。但GPU本身并非是专门为AI计算而设计的芯片，其中有大量的逻辑计算对于AI算法来说毫无用处，所以行业自然也需要专门针对AI算法的专用AI芯片。</p>
<p>近几年，全球已经有上百家公司投入到新型AI芯片的研发和设计当中，当然最终能够成功流片并推出商用的仍然是几家巨头公司和少数实力雄厚的独角兽公司。</p>
<p>这其中，2017年初创成立的Graphcore所研发的AI芯片IPU，则成为这些AI芯片当中的另类代表，因其不同于GPU架构的创新得到了业内的关注。而这正是我们要着重介绍的部分。</p>
<p>更适合AI计算的IPU芯片</p>
<p>近两年，AI 芯片出现了各种品类的井喷，其中甚至出现一些堪称疯狂的另类产品。</p>
<p>比如一家同样创立四年的AI芯片创业公司Cerebras Systems就发布了史上最大的半导体芯片Wafer Scale Engine（WSE），号称“晶圆级发动机”，拥有1.2万亿个晶体管，比英伟达最大的GPU要大出56.7倍。这块芯片主要瞄准的是超级计算和和大型云计算中心市场，其创新之处在于一体化的芯片设计大幅提高了内部的数据通信数据，但其架构仍然类似于GPU的计算架构。</p>
<p>而Graphcore的 IPU与GPU的架构差异非常大，代表的是一种新的技术架构，可以说是专门为解决CPU和GPU在AI计算中难以解决的问题而设计的。</p>
<p><img src="http://p1.itc.cn/q_70/images03/20201005/fea982496c294ba7bb896aedf051cf62.png" /></p>
<p>IPU为AI计算提供了全新的技术架构，同时将训练和推理合二为一，兼具处理二者工作的能力。</p>
<p>我们以目前已经量产的IPU的GC2处理器来看，IPU GC2采用台积电的16nm工艺，拥有 236亿个晶体管，在120瓦的功耗下有125TFlops的混合精度，另外有45TB/s内存的带宽、8TB/s片上多对多交换总线，2.5 TB/s的片间IPU-Links。</p>
<p>其中，片内有1216个IPU-Tiles独立处理器核心，每个Tile中有独立的IPU核，作为计算以及In-Processor-Memory（处理器内的内存）。对整个GC2来说共有7296个线程（每个核心最多可以跑6个线程），能够支持7296个程序并行运行，处理器内的内存总共可以达到300MB，其设计思路就是要把所有模型放在片内处理。</p>
<p>首先，IPU作为一个标准的神经网络处理芯片，可以支持多种神经网络模型，因其具备数以千计到数百万计的顶点数量，远远超过GPU的顶点规模，可以进行更高潜力的并行计算工作。此外，IPU的顶点的稀疏特性，令其也可以高效处理GPU不擅长的稀疏的卷积计算。其次，IPU 也支持了模型参数的复用，这些复用特性可以获取数据中的空间或时间不变性，对于训练与推理的性能会有明显帮助。</p>
<p>其次，为解决芯片内存的宽带限制，IPU采用了大规模并行MIMD（多指令流多数据流）众核架构，同时，IPU架构做了大规模分布式的片上SRAM。片内300MB的SRAM，相对于GPU的GDDR、HBM来说，可以做到数十倍的性能提升，而且与访问外存相比，SRAM的片内时延基本可以忽略不计。</p>
<p>最后，IPU采用了高效的多核通信技术BSP（Bulk Synchronous Parallel）。IPU是目前世界上第一款采用BSP通信的处理器，支持内部1216个核心之间的通信以及跨不同的IPU之间的通信。通过硬件支持BSP协议，并通过BSP协议把整个计算逻辑分成了计算、同步、交换，能极大方便工程师们的开发工作。</p>
<p><img src="http://p1.itc.cn/q_70/images03/20201005/b2163ce8521b4f4bbf03bdeb3a99ecad.png" /></p>
<p>基于以上IPU的差异化特点，IPU在某些批量训练和推理中能够获得更好的性能、更低延时和更快网络收敛。片内的SRAM相对于片外存储，也有高带宽和低延时的优势。</p>
<p>今年7月，Graphcore发布了二代的Colossus MK2 IPU (MK2)，以及包含四颗MK2芯片系统方案的IPU-Machine：M2000 (IPU-M2000)，其核心数增加了20% ，达到1472个，8832个可并行执行的线程。片内SRAM则多出3倍，增加到900MB，互联扩展性能是上一代的16倍。显然在计算、数据和通信扩展层面，MK2都算是延续了第一代IPU堆料狂魔的作风。</p>
<p><img src="http://p9.itc.cn/q_70/images03/20201005/bf9c084564fe4ac8868753217aceffad.png" /></p>
<p>由4个IPU芯片构成的IPU-M2000系统，可以提供大约1 PetaFLOPs的算力。基于IPU的多层级存储结构，与IPU Exchange Memory等技术优化，整体与GPU的HBM2存储比较，可以提供超过100倍的带宽以及大约10倍的容量，可以适用于更复杂的AI模型和程序。</p>
<p><img src="http://p7.itc.cn/q_70/images03/20201005/f96993660e31462ba12c405a0ffdd0ed.png" /></p>
<p>计算加上数据的突破可以让IPU在原生稀疏计算中展现出领先GPU 10-50倍的性能优势，在通信上，Graphcore专为为AI横向扩展设计了IPU-Fabric，解决数据中心大规模计算横向扩展的关键问题。Graphcore将计算、数据、通信三者的突破技术结合，构建了大规模可扩展的IPU-POD系统，最终可以提供一个AI计算和逻辑进行解耦、系统易于部署、超低网络延时、高可靠的AI超算集群。</p>
<p>可以预计，未来IPU在各类AI应用中将具有更大的优势，而这也必然会引起英伟达的注意。那么，相较于英伟达GPU所占据的AI行业生态位的霸主地位，IPU会有哪些前景，也会遭遇哪些困境呢？</p>
<p>走向通用AI计算的“另辟蹊径”</p>
<p>如果回顾下AI芯片的发展经历，我们看到在经过这几年专用AI芯片的井喷之后，也开始面临一个尴尬困境，那就是ASIC芯片的灵活性或者说可编程性很差，对应的专用AI芯片只能应对一种算法应用，而算法本身则在3-6个月的时间就有可能变化一次，或许出现很多AI芯片还未上市，算法就已经发生进化的问题，一些AI芯片注定无法生产。当然，专用AI芯片的优势也很明显，在性能、功耗和效率上远胜更加通用的GPU，对于一些非常具体的AI应用场景，这些专用芯片就具有了巨大的收益。</p>
<p>从专注图像渲染崛起的英伟达的GPU，走的也是相当于ASIC的技术路线，但随着游戏、视频渲染以及AI加速需要的出现，英伟达的GPU也在向着GPGPU（General Purpose GPU）的方向演进。为保持其在GPU领域的寡头地位，使得英伟达必须一直保持先进的制程工艺，保持其通用性，但是要牺牲一定的效能优势。</p>
<p>这给后来者一定的启发，那就是AI芯片既要具备一定的灵活的可编程性（通用性），又要具备专用的高效性能优势。这为IPU找到了一个新的细分市场，也就是介入GPU不能很好发挥效能的神经网络模型，比如强化学习等类型，同时又避免的专用AI芯片的不可扩展性，能够部署在更大规模的云计算中心或超算中心，对新算法模型保持足够的弹性计算空间。</p>
<p>目前来看，IPU正在成为仅次于GPU和谷歌TPU的第三大部署平台，基于IPU的应用已经覆盖包括自然语言处理、图像/视频处理、时序分析、推荐/排名及概率模型等机器学习的各个应用场景。</p>
<p>典型的如通过IPU可以训练胸片，帮助医学人员快速进行新冠肺炎的诊断；如在金融领域，对涉及算法交易、投资管理、风险管理及诈骗识别的场景进行更快的分析和判断；此外在生命科学领域、通信网络等方面，都可以同IPU实现高于GPU性能的AI加速。</p>
<p><img src="http://p4.itc.cn/q_70/images03/20201005/65be68801a634e0ebc60d1c342a06e5f.png" /></p>
<p>（NLP模型参数的指数增长）</p>
<p>当然，IPU想要在AI计算中拥有挑战GPU地位的资格，除了在性能和价格上面证明自己的优势之外，还需要在为机器学习框架提供的软件栈上提供更多选择，获得主流AI算法厂商的支持，在标准生态、操作系统上也需要有广泛的支持，对于开发者有更方便的开发工具和社区内容的支持，才能从实际应用中壮大IPU的开发生态。</p>
<p>今年， AI芯片产业正在遭遇洗牌期，一些AI芯片企业黯然退场，但这并不意味着AI计算遭遇寒冬，反而AI算力正在得到大幅提升，以今年数量级提升GPT-3的出场就可以看出这样的趋势。</p>
<p>一个AI芯片从产出到大规模应用必须要经过一系列的中间环节，包括像上面提到的支持主流算法框架的软件库、工具链、用户生态等等，打通这样一条链条都会面临一个巨大挑战。</p>
<p>现在，GPU已经形成一个非常完整的AI算力生态链路，而IPU则仍然在路上，是否能真正崛起，还需要整个AI产业和开发者用实际行动来投票。</p>