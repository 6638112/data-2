➜可以估计不确定性的神经网络——SDE-Net模型浅析
http://zhuanlan.zhihu.com/p/234834189?utm_campaign=rss&utm_medium=rss&utm_source=rss&utm_content=title	47708
<p>随着深度学习技术的不断发展，DNN模型的预测能力变得越来越强，然而在一些情况下这却并不是我们想要的，比如说给模型一个与训练集完全不相关的测试样本，我们希望模型能够承认自己的“<b>无知</b>”，而不是强行给出一个预测结果，这种能力对于自动驾驶或者医疗诊断等重视风险的任务是至关重要的。因此，为了达到这个目的，我们的模型需要具有<b>量化不确定性</b>的能力，对于那些它没有把握的样本，模型应该给出较高的不确定性，这样就能指导我们更好地利用模型的预测结果。</p><p>之前我介绍过可以预测概率分布的DeepAR模型，其实这次介绍的SDE-Net与它的目标是一致的，都是令模型<b>在预测的基础上还能够度量预测结果的不确定性</b>，不过SDE-Net的实现这个目标的思路与DeepAR不同，下面就来具体介绍。</p><a href="https://zhuanlan.zhihu.com/p/201030350" data-draft-node="block" data-draft-type="link-card" data-image="https://picb.zhimg.com/v2-623c7fedb83e9a51e7fdf2b8ff9ad5f1_180x120.jpg" data-image-width="880" data-image-height="629" class="internal">段易通：概率自回归预测——DeepAR模型浅析</a><h2>不确定性</h2><p>上文中已经提到，我们的目的是要量化不确定性，那么我们当然要先知道是什么导致了模型的不确定性、并且要了解不确定性产生的来源有哪些，论文中认为模型预测的不确定性来自于两个方面：</p><ul><li>aleatoric uncertainty：来自于任务本身所固有的自然随机性（比如说label噪声等）</li><li>epistemic uncertainty：由于缺乏训练数据所导致的，模型对于训练数据分布之外的样本是无知的</li></ul><p>对于aleatoric uncertainty，它是由任务本身天然决定的，可以设想一个<b>所有标签都是噪声</b>的训练集，用这样的数据集训练出来的模型，它的预测结果显然是不可信的，即不确定度很大；而对于epistemic uncertainty，它是由于<b>模型的认知不足</b>造成的，在面对训练集分布之外的数据时，模型的预测结果会具有较高的不确定度。</p><p>下图是对两种不确定度对模型预测结果影响的示意图，我们这里用到的是概率模型（比如说DeepAR或者后面要说的SDE-Net，输出的是一个随机变量而不是一个定值，因此通过模型得到的其实是一个概率分布）。其中左边的simplex corner代表分类任务（三分类），右边的二维坐标代表回归任务，其中横轴代表predictive mean <img src="https://www.zhihu.com/equation?tex=%5Cmu%28x%5E%7B%2A%7D%29" alt="\mu(x^{*})" eeimg="1"/> 、纵轴是predictive variance <img src="https://www.zhihu.com/equation?tex=%5Csigma%28x%5E%7B%2A%7D%29" alt="\sigma(x^{*})" eeimg="1"/> （不太明白这里<img src="https://www.zhihu.com/equation?tex=%5Cmu%28x%5E%7B%2A%7D%29" alt="\mu(x^{*})" eeimg="1"/>和 <img src="https://www.zhihu.com/equation?tex=%5Csigma%28x%5E%7B%2A%7D%29" alt="\sigma(x^{*})" eeimg="1"/> 的含义，求解惑~）。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-857c6282bb1c9dacb3ee4cbfdfdc7fc2_b.jpg" data-caption="" data-size="normal" data-rawwidth="1240" data-rawheight="230" class="origin_image zh-lightbox-thumb" width="1240" data-original="https://pic1.zhimg.com/v2-857c6282bb1c9dacb3ee4cbfdfdc7fc2_r.jpg"/></figure><h2>SDE量化不确定性</h2><p>我们知道，神经网络尤其是ResNet可以看做是由常微分方程（ODE）控制的一个<b>动力学系统</b>（具体可以看ResNet的相关资料，或者我的<a href="https://zhuanlan.zhihu.com/p/92254686" class="internal">这篇文章</a>），相邻层之间的输入输出关系为：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D_%7Bt%2B1%7D%3D%5Cboldsymbol%7Bx%7D_%7Bt%7D%2Bf%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bt%7D%2C+t%5Cright%29%5C%5C" alt="\boldsymbol{x}_{t+1}=\boldsymbol{x}_{t}+f\left(\boldsymbol{x}_{t}, t\right)\\" eeimg="1"/> </p><p>其中 <img src="https://www.zhihu.com/equation?tex=x_t" alt="x_t" eeimg="1"/> 是第t层的隐藏状态，如果我们令 <img src="https://www.zhihu.com/equation?tex=%5CDelta+t+%3D1" alt="\Delta t =1" eeimg="1"/> ，上式可以写做： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cboldsymbol%7Bx%7D_%7Bt%2B%5CDelta+t%7D-%5Cboldsymbol%7Bx%7D_%7Bt%7D%7D%7B%5CDelta+t%7D%3Df%28x_t+%2Ct%29" alt="\frac{\boldsymbol{x}_{t+\Delta t}-\boldsymbol{x}_{t}}{\Delta t}=f(x_t ,t)" eeimg="1"/> ，如果我们让 <img src="https://www.zhihu.com/equation?tex=%5CDelta+t+%5Crightarrow+0" alt="\Delta t \rightarrow 0" eeimg="1"/> ，那么就有：</p><p><img src="https://www.zhihu.com/equation?tex=%5Clim+_%7B%5CDelta+%5Crightarrow+0%7D+%5Cfrac%7Bx_%7Bt%2B%5CDelta+t%7D-x_%7Bt%7D%7D%7B%5CDelta+t%7D%3D%5Cfrac%7Bd+x_%7Bt%7D%7D%7Bd+t%7D%3Df%5Cleft%28x_%7Bt%7D%2C+t%5Cright%29+%5CLongleftrightarrow+d+x_%7Bt%7D%3Df%5Cleft%28x_%7Bt%7D%2C+t%5Cright%29+d+t" alt="\lim _{\Delta \rightarrow 0} \frac{x_{t+\Delta t}-x_{t}}{\Delta t}=\frac{d x_{t}}{d t}=f\left(x_{t}, t\right) \Longleftrightarrow d x_{t}=f\left(x_{t}, t\right) d t" eeimg="1"/> </p><p>ResNet其实可以看做是离散化的动力学系统，不过控制方程是一个ODE，所以神经网络得到是只是一个确定性结果。为了让模型可以估计不确定性，我们自然就想到是不是可以改<b>用一个随机微分方程（SDE）来控制dynamic呢</b>？这就是论文的核心思想，其实也很简单，就是在原有ODE的基础上再加上一个随机项，这里采用的是标准布朗运动，那么dynamic的形式就变为：</p><p><img src="https://www.zhihu.com/equation?tex=d+%5Cboldsymbol%7Bx%7D_%7Bt%7D%3Df%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bt%7D%2C+t%5Cright%29+d+t%2Bg%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bt%7D%2C+t%5Cright%29+d+W_%7Bt%7D%5C%5C" alt="d \boldsymbol{x}_{t}=f\left(\boldsymbol{x}_{t}, t\right) d t+g\left(\boldsymbol{x}_{t}, t\right) d W_{t}\\" eeimg="1"/> </p><p>这里 <img src="https://www.zhihu.com/equation?tex=W_t" alt="W_t" eeimg="1"/> 就是标准布朗运动，可以看出函数 <img src="https://www.zhihu.com/equation?tex=g" alt="g" eeimg="1"/> 控制的就是dynamic的波动，我们就用它来代表模型对于epistemic uncertainty的估计，下图是 <img src="https://www.zhihu.com/equation?tex=g" alt="g" eeimg="1"/> 的大小对于dynamic的影响。</p><figure data-size="small"><img src="https://pic1.zhimg.com/v2-238239fa07583c211bc7f4fd6b536651_b.jpg" data-caption="" data-size="small" data-rawwidth="1103" data-rawheight="563" class="origin_image zh-lightbox-thumb" width="1103" data-original="https://pic1.zhimg.com/v2-238239fa07583c211bc7f4fd6b536651_r.jpg"/></figure><h2>模型构造</h2><p>这样一来，我们就利用SDE来描述了隐层状态的dynamic，并通过随机过程的方差来量化估计epistemic uncertainty。为了使模型具有良好的预测精度和可靠的不确定性估计，论文的SDE-Net模型用了两个单独的神经网络来表示分别dynamic的<b>漂移</b>和<b>扩散</b>，如下图所示：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-dc2686f9e4f8e3466e171562fbca4bcf_b.jpg" data-caption="" data-size="normal" data-rawwidth="1184" data-rawheight="390" class="origin_image zh-lightbox-thumb" width="1184" data-original="https://pic1.zhimg.com/v2-dc2686f9e4f8e3466e171562fbca4bcf_r.jpg"/></figure><p>可以看出，对于分布内的测试样本，diffusion net计算出的不确定度很小，因此drift net占主导地位，我们可以获得置信度很高的预测结果；但是对于分布外的样本，计算出的不确定度很大，因此diffusion net占主导地位，得到的结果几乎就是随机分布的结果。</p><p>对于SDE-Net的两个神经网络，论文采用了如下的目标函数来进行训练</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D+%5Cmin+_%7B%5Cboldsymbol%7B%5Ctheta%7D_%7Bf%7D%7D+%5Cmathrm%7BE%7D_%7B%5Cboldsymbol%7Bx%7D_%7B0%7D+%5Csim+P_%7B%5Ctext+%7Btrain+%7D%7D%7D+%5Cmathrm%7BE%7D%5Cleft%28L%5Cleft%28%5Cboldsymbol%7Bx%7D_%7BT%7D%5Cright%29%5Cright%29%2B%5Cmin+_%7B%5Cboldsymbol%7B%5Ctheta%7D_%7Bg%7D%7D+%5Cmathrm%7BE%7D_%7B%5Cboldsymbol%7Bx%7D_%7B0%7D+%5Csim+P_%7B%5Ctext+%7Btrain+%7D%7D%7D+g%5Cleft%28%5Cboldsymbol%7Bx%7D_%7B0%7D+%3B+%5Cboldsymbol%7B%5Ctheta%7D_%7Bg%7D%5Cright%29+%2B%5Cmax+_%7B%5Cboldsymbol%7B%5Ctheta%7D_%7Bg%7D%7D+%5Cmathrm%7BE%7D_%7B%5Ctilde%7B%5Cboldsymbol%7Bx%7D%7D_%7B0%7D+%5Csim+%5Coperatorname%7BP_%7BOOD%7D%7D%7D+g%5Cleft%28%5Ctilde%7B%5Cboldsymbol%7Bx%7D%7D_%7B0%7D+%3B+%5Cboldsymbol%7B%5Ctheta%7D_%7Bg%7D%5Cright%29+%5C%5C+%5Ctext+%7B+s.t.+%7D+%5Cquad+d+%5Cboldsymbol%7Bx%7D_%7Bt%7D%3D%5Cunderbrace%7Bf%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bt%7D%2C+t+%3B+%5Cboldsymbol%7B%5Ctheta%7D_%7Bf%7D%5Cright%29%7D_%7B%5Ctext+%7Bdrift+neural+net+%7D%7D+d+t%2B%5Cunderbrace%7Bg%5Cleft%28%5Cboldsymbol%7Bx%7D_%7B0%7D+%3B+%5Cboldsymbol%7B%5Ctheta%7D_%7Bg%7D%5Cright%29%7D_%7B%5Ctext+%7Bdiffusion+neural+net+%7D%7D+d+W_%7Bt%7D+%5Cend%7Barray%7D" alt="\begin{array}{l} \min _{\boldsymbol{\theta}_{f}} \mathrm{E}_{\boldsymbol{x}_{0} \sim P_{\text {train }}} \mathrm{E}\left(L\left(\boldsymbol{x}_{T}\right)\right)+\min _{\boldsymbol{\theta}_{g}} \mathrm{E}_{\boldsymbol{x}_{0} \sim P_{\text {train }}} g\left(\boldsymbol{x}_{0} ; \boldsymbol{\theta}_{g}\right) +\max _{\boldsymbol{\theta}_{g}} \mathrm{E}_{\tilde{\boldsymbol{x}}_{0} \sim \operatorname{P_{OOD}}} g\left(\tilde{\boldsymbol{x}}_{0} ; \boldsymbol{\theta}_{g}\right) \\ \text { s.t. } \quad d \boldsymbol{x}_{t}=\underbrace{f\left(\boldsymbol{x}_{t}, t ; \boldsymbol{\theta}_{f}\right)}_{\text {drift neural net }} d t+\underbrace{g\left(\boldsymbol{x}_{0} ; \boldsymbol{\theta}_{g}\right)}_{\text {diffusion neural net }} d W_{t} \end{array}" eeimg="1"/> </p><p>其中 <img src="https://www.zhihu.com/equation?tex=L%28%5Ccdot%29" alt="L(\cdot)" eeimg="1"/> 是任务的损失函数，T是随机过程的末时刻（即网络的输出层）， <img src="https://www.zhihu.com/equation?tex=P_%7Btrain%7D" alt="P_{train}" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=P_%7BOOD%7D" alt="P_{OOD}" eeimg="1"/> 分别是训练集分布内与分布外（OOD）的数据，OOD数据可以通过给原数据加噪声做变换的方式获得，也可以直接用另一个任务目标不相关的数据集。</p><p>可以看出，目标函数一共分为三部分，前两项是关于分布内样本的目标函数，其目的是<b>保证在常规的损失最小化的基础上，还要使得这些样本的不确定度估计较小</b>，后一项是关于OOD数据的，对于这些样本，我们<b>不关注其loss的大小，而是只令模型对于OOD样本的不确定度增加</b>。</p><p>需要注意的是，这里SDE-Net中每一层的参数都是共享的，而且扩散项的方差仅由起始点x0决定，这样可以使模型训练起来更容易。</p><p>训练好模型之后，我们可以通过多次采样的方法，来得到多个输出 <img src="https://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cboldsymbol%7Bx%7D_%7BT%7D%5Cright%5C%7D_%7Bm%3D1%7D%5E%7BM%7D" alt="\left\{\boldsymbol{x}_{T}\right\}_{m=1}^{M}" eeimg="1"/> ，这种采样计算的思路与传统的集成方法具有相似之处，但是传统方法需要训练多个模型，而SDE-Net<b>只需训练一次即可通过布朗运动的随机性得到多个输出样本</b>，从而大大减小了训练成本。</p><h2>理论分析</h2><p>论文还对模型做了一些理论分析，内容不多，就直接放原文了</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-8a4e778e2f5d8736b948668cab7aaca8_b.jpg" data-caption="" data-size="normal" data-rawwidth="742" data-rawheight="518" class="origin_image zh-lightbox-thumb" width="742" data-original="https://pic1.zhimg.com/v2-8a4e778e2f5d8736b948668cab7aaca8_r.jpg"/></figure><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-71468bede83d01c6157690cfa9151e66_b.jpg" data-caption="" data-size="normal" data-rawwidth="743" data-rawheight="338" class="origin_image zh-lightbox-thumb" width="743" data-original="https://pic1.zhimg.com/v2-71468bede83d01c6157690cfa9151e66_r.jpg"/></figure><h2>模型训练</h2><p>考虑到模型的层数是有限的，因此我们需要将SDE离散化，形式如下：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D_%7Bk%2B1%7D%3D%5Cboldsymbol%7Bx%7D_%7Bk%7D%2Bf%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bk%7D%2C+t+%3B+%5Cboldsymbol%7B%5Ctheta%7D_%7Bf%7D%5Cright%29+%5CDelta+t%2Bg%5Cleft%28%5Cboldsymbol%7Bx%7D_%7B0%7D+%3B+%5Cboldsymbol%7B%5Ctheta%7D_%7Bg%7D%5Cright%29+%5Csqrt%7B%5CDelta+t%7D+Z_%7Bk%7D" alt="\boldsymbol{x}_{k+1}=\boldsymbol{x}_{k}+f\left(\boldsymbol{x}_{k}, t ; \boldsymbol{\theta}_{f}\right) \Delta t+g\left(\boldsymbol{x}_{0} ; \boldsymbol{\theta}_{g}\right) \sqrt{\Delta t} Z_{k}" eeimg="1"/> </p><p>其中时间区间为 <img src="https://www.zhihu.com/equation?tex=%5B0%2CT%5D" alt="[0,T]" eeimg="1"/> ，模型一共有N层，因此 <img src="https://www.zhihu.com/equation?tex=%5CDelta+t%3DT+%2F+N" alt="\Delta t=T / N" eeimg="1"/> 。</p><p>总的来看，SDE-Net的训练算法如下：</p><figure data-size="small"><img src="https://picb.zhimg.com/v2-380465b8fd52b397c218dda415d47e5e_b.jpg" data-caption="" data-size="small" data-rawwidth="737" data-rawheight="885" class="origin_image zh-lightbox-thumb" width="737" data-original="https://picb.zhimg.com/v2-380465b8fd52b397c218dda415d47e5e_r.jpg"/></figure><p>简单概括一下这个算法，首先我们从分布内采样出一批训练数据，然后通过一个降采样层得到输入 <img src="https://www.zhihu.com/equation?tex=X_%7B0%7D%5E%7BN_M%7D" alt="X_{0}^{N_M}" eeimg="1"/> ，接着就根据SDE-Net来控制隐层状态的dynamic，并在最后接一个全连接层得到模型的输出 <img src="https://www.zhihu.com/equation?tex=X_%7Bf%7D%5E%7BN_M%7D" alt="X_{f}^{N_M}" eeimg="1"/> ，这样我们就可以通过计算loss的梯度来更新降采样层、drift net以及全连接层的参数；另外，我们还要从分布外采样出一批数据（OOD数据），然后根据分布内外的数据分别对diffusion net的参数做梯度下降和梯度上升。</p><h2>实验</h2><p>论文的实验研究了不确定性估计在model robustness和label efficiency中的作用，实验采用的对比模型有：Threshold、MC-dropout、DeepEnsemble、Prior network（PN）、Bayes by Backpropagation (BBP)、preconditioned Stochastic gradient Langevin dynamics(p-SGLD)；其中PN和SDE-Net需要额外的OOD数据，这里通过对原有的数据样本上加上高斯噪声来进行构造、或者直接采用另一个数据集，至于其它的一些具体设定可以看论文的实验和补充材料部分。</p><p><b>1.OOD检测</b></p><p>就像在文章开头提到的，让模型有“自知之明”是非常重要的，因此第一个任务就是评估模型识别OOD样本的能力，实验中使用的metric如下所示，这些metrics都是值越大越好：</p><ol><li>True negative rate (TNR) at 95% true positive rate (TPR)</li><li>Area under the receiver operating characteristic curve (AUROC)</li><li>Area under the precision-recall curve (AUPR)</li><li>Detection accuracy</li></ol><p>实验结果如下所示：</p><figure data-size="small"><img src="https://picb.zhimg.com/v2-d1e89744ad8cc9b32f6e74a83159f5b9_b.jpg" data-size="small" data-rawwidth="1119" data-rawheight="628" class="origin_image zh-lightbox-thumb" width="1119" data-original="https://picb.zhimg.com/v2-d1e89744ad8cc9b32f6e74a83159f5b9_r.jpg"/><figcaption>分类任务</figcaption></figure><figure data-size="small"><img src="https://pic1.zhimg.com/v2-e1356ea4296cbc9fee9aca736e869b4a_b.jpg" data-size="small" data-rawwidth="1151" data-rawheight="463" class="origin_image zh-lightbox-thumb" width="1151" data-original="https://pic1.zhimg.com/v2-e1356ea4296cbc9fee9aca736e869b4a_r.jpg"/><figcaption>回归任务</figcaption></figure><p>从表中可以看出，SDE-Net的性能基本超越了其它所有模型。另外，下图是提高模型层数或者ensemble数量对OOD检测的影响，可以看出SDE-Net不需要像一些其它模型那样必须大量堆叠才能达到最优性能。</p><figure data-size="small"><img src="https://pic1.zhimg.com/v2-af8d5dbf8ad572dabf7f80aca0ba8005_b.jpg" data-caption="" data-size="small" data-rawwidth="1081" data-rawheight="608" class="origin_image zh-lightbox-thumb" width="1081" data-original="https://pic1.zhimg.com/v2-af8d5dbf8ad572dabf7f80aca0ba8005_r.jpg"/></figure><p><b>2.误分类检测</b></p><p>如果模型预测的不确定性很大，那么就说明模型对预测结果是没有把握的，样本可能被分类错误。因此这个任务的目的是利用预测的不确定性来找出模型分类错误的样本，其结果如下：</p><figure data-size="small"><img src="https://pic1.zhimg.com/v2-b0db75e28b1cee3cc57e09d615e97ed9_b.jpg" data-caption="" data-size="small" data-rawwidth="804" data-rawheight="530" class="origin_image zh-lightbox-thumb" width="804" data-original="https://pic1.zhimg.com/v2-b0db75e28b1cee3cc57e09d615e97ed9_r.jpg"/></figure><p>虽然P-SGLD的效果也不错，不过它的计算成本很高，因此在实际情况中SDE-Net可能会是一个更好的选择。</p><p><b>3.对抗样本检测</b></p><p>我们知道，在样本中加入一些很小的对抗扰动后，正常的DNNs会变得非常容易出错，因此这个任务的目标就是从样本集中找出对抗样本，这里采用了两种对抗攻击方式Fast Gradient-Sign Method (FGSM)和Projected Gradient Descent (PGD)来产生对抗样本，实验结果如下：</p><figure data-size="small"><img src="https://pic1.zhimg.com/v2-0577f35067aabeacd2fc324f22833e7a_b.jpg" data-caption="" data-size="small" data-rawwidth="1156" data-rawheight="664" class="origin_image zh-lightbox-thumb" width="1156" data-original="https://pic1.zhimg.com/v2-0577f35067aabeacd2fc324f22833e7a_r.jpg"/></figure><figure data-size="small"><img src="https://pic1.zhimg.com/v2-b2e224c586e65774c37a023998b6379f_b.jpg" data-caption="" data-size="small" data-rawwidth="1148" data-rawheight="663" class="origin_image zh-lightbox-thumb" width="1148" data-original="https://pic1.zhimg.com/v2-b2e224c586e65774c37a023998b6379f_r.jpg"/></figure><p><b>4.主动学习</b></p><p>假设一开始样本集里有标注的样本很少，模型需要自己挑一些信息量大的样本出来让专家进行标注，这就是主动学习的思想。直观上来看，<b>挑选信息量大的样本可以显着减少用于模型训练的数据量</b>，而信息量小的样本会增加训练成本、甚至会导致过拟合。最后一个任务就是关于主动学习的，论文设定acquisition function（不了解的同学可以学习一下相关知识点）的形式为：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cboldsymbol%7Bx%7D_%7B%5Cmathrm%7Bnew%7D%7D%2C+y_%7B%5Cmathrm%7Bnew%7D%7D%5Cright%5C%7D+%5Csim+p_%7B%5Cmathrm%7Bnew%7D%7D%28%5Cboldsymbol%7Bx%7D%2C+y%29+%5Cpropto%5Cleft%281%2B%5Cfrac%7B%5Coperatorname%7BVar%7D%5B%5Cmu%28%5Cboldsymbol%7Bx%7D%29%5D%7D%7B%5Csigma%5E%7B2%7D%28%5Cboldsymbol%7Bx%7D%29%7D%5Cright%29%5E%7B2%7D" alt="\left\{\boldsymbol{x}_{\mathrm{new}}, y_{\mathrm{new}}\right\} \sim p_{\mathrm{new}}(\boldsymbol{x}, y) \propto\left(1+\frac{\operatorname{Var}[\mu(\boldsymbol{x})]}{\sigma^{2}(\boldsymbol{x})}\right)^{2}" eeimg="1"/> </p><p>该式的意思就是让模型选择那些具有较高的epistemic uncertainty但数据具有较低的low aleatoric noise的样本，结果如下，可以看出SDE-Net选择的样本都是信息量比较大样本，因此RMSE下降的更快。</p><figure data-size="small"><img src="https://pic1.zhimg.com/v2-4b89dd548c8a9da8699adf30cdf2fcf4_b.jpg" data-caption="" data-size="small" data-rawwidth="796" data-rawheight="525" class="origin_image zh-lightbox-thumb" width="796" data-original="https://pic1.zhimg.com/v2-4b89dd548c8a9da8699adf30cdf2fcf4_r.jpg"/></figure><h2>总结</h2><p>ResNet可以对应为一个离散ODE，这篇文章受到该思路的启发，构建了一个可以被看做离散SDE的SDE-Net模型，模型由两个神经网络drift net和diffusion net构成，其中drift net与传统模型类似，是为了预测模型的输出结果，而diffusion net则用来估计预测的不确定性，估计出的不确定性可以应用于OOD样本检测、误分类检测、主动学习等多个任务，而可以估计不确定性的SDE-Net也更加适合于一些关注风险的实际应用领域。</p><p>个人感觉，论文中通过SDE来评估不确定性的想法很有意思，确实有一定的可取之处；不过模型为了训练diffusion net网络，专门构建了用于梯度上升的OOD数据集，这样的数据集无论怎么构建，都很难代表整个训练集以外的分布，因此不可避免地会引入一些bias，而这就可能会影响模型对于不确定度的估计。</p><p class="ztext-empty-paragraph"><br/></p><h2>参考文献</h2><p>[1] <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2008.10546%3Fcontext%3Dstat" class=" wrap external" target="_blank" rel="nofollow noreferrer">SDE-Net: Equipping Deep Neural Networks with Uncertainty Estimates</a></p><p></p>